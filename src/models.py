import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, BatchNorm
import numpy as np
from typing import Optional, Tuple, Dict
from collections import deque


# ============================================================
# ğŸ”§ è¾…åŠ©å‡½æ•°ï¼šæ„å»ºå› æœæ©ç 
# ============================================================
def build_causal_masks(edge_index: np.ndarray, num_nodes: int, 
                       source_node: int = 0,
                       admittance_matrix: Optional[np.ndarray] = None) -> Dict[str, torch.Tensor]:
    """
    æ„å»ºå› æœçº¦æŸæ©ç 

    Args:
        edge_index: (2, E) è¾¹ç´¢å¼•æ•°ç»„
        num_nodes: èŠ‚ç‚¹æ€»æ•°
        source_node: æºèŠ‚ç‚¹ï¼ˆå˜å‹å™¨/é¦ˆçº¿èµ·ç‚¹ï¼‰
        admittance_matrix: å¯¼çº³çŸ©é˜µï¼ˆå¯é€‰ï¼Œç”¨äºåˆå§‹åŒ–å› æœå¼ºåº¦ï¼‰

    Returns:
        dict: {
            'adj_mask': (N, N) é‚»æ¥æ©ç ï¼Œåªæœ‰ç‰©ç†ç›¸é‚»çš„èŠ‚ç‚¹æ‰æœ‰å› æœå…³ç³»
            'direction_mask': (N, N) æ–¹å‘æ©ç ï¼Œå› æœåªèƒ½ä»ä¸Šæ¸¸æµå‘ä¸‹æ¸¸
            'initial_causal': (N, N) åˆå§‹å› æœå¼ºåº¦ï¼ˆåŸºäºå¯¼çº³ï¼‰
            'node_depths': (N,) æ¯ä¸ªèŠ‚ç‚¹åˆ°æºçš„æ·±åº¦
        }
    """
    # 1. æ„å»ºé‚»æ¥è¡¨
    adj_list = {i: set() for i in range(num_nodes)}
    for i in range(edge_index.shape[1]):
        src, dst = int(edge_index[0, i]), int(edge_index[1, i])
        if src < num_nodes and dst < num_nodes:
            adj_list[src].add(dst)
            adj_list[dst].add(src)

    # 2. BFS è®¡ç®—èŠ‚ç‚¹æ·±åº¦ï¼ˆè·ç¦»æºèŠ‚ç‚¹çš„è·³æ•°ï¼‰
    node_depths = torch.full((num_nodes,), float('inf'))
    
    # ç¡®ä¿æºèŠ‚ç‚¹åœ¨æœ‰æ•ˆèŒƒå›´å†…
    if source_node >= num_nodes:
        source_node = 0
    
    node_depths[source_node] = 0
    queue = deque([source_node])
    visited = {source_node}

    while queue:
        node = queue.popleft()
        for neighbor in adj_list[node]:
            if neighbor not in visited:
                visited.add(neighbor)
                node_depths[neighbor] = node_depths[node] + 1
                queue.append(neighbor)

    # å¤„ç†æœªè¿æ¥çš„èŠ‚ç‚¹ï¼ˆè®¾ç½®ä¸ºæœ€å¤§æ·±åº¦ï¼‰
    max_depth = node_depths[node_depths != float('inf')].max() if (node_depths != float('inf')).any() else 0
    node_depths[node_depths == float('inf')] = max_depth + 1

    # 3. æ„å»ºé‚»æ¥æ©ç ï¼ˆ1 è¡¨ç¤ºæœ‰è¾¹ç›¸è¿ï¼‰
    adj_mask = torch.zeros(num_nodes, num_nodes)
    for i in range(edge_index.shape[1]):
        src, dst = int(edge_index[0, i]), int(edge_index[1, i])
        if src < num_nodes and dst < num_nodes:
            adj_mask[src, dst] = 1.0
            adj_mask[dst, src] = 1.0
    
    # æ·»åŠ è‡ªç¯
    adj_mask += torch.eye(num_nodes)
    adj_mask = torch.clamp(adj_mask, 0, 1)

    # 4. æ„å»ºæ–¹å‘æ©ç ï¼ˆåªæœ‰ä¸Šæ¸¸â†’ä¸‹æ¸¸æˆ–åŒå±‚æ‰å…è®¸ï¼‰
    # direction_mask[i, j] = 1 è¡¨ç¤º j å¯ä»¥å½±å“ iï¼ˆj çš„æ·±åº¦ <= i çš„æ·±åº¦ï¼‰
    direction_mask = torch.zeros(num_nodes, num_nodes)
    for i in range(num_nodes):
        for j in range(num_nodes):
            if node_depths[j] <= node_depths[i]:
                direction_mask[i, j] = 1.0

    # 5. åˆå§‹åŒ–å› æœå¼ºåº¦ï¼ˆåŸºäºå¯¼çº³æˆ–å‡åŒ€åˆ†å¸ƒï¼‰
    if admittance_matrix is not None and admittance_matrix.shape == (num_nodes, num_nodes):
        # ä½¿ç”¨å¯¼çº³ä½œä¸ºåˆå§‹å› æœå¼ºåº¦
        initial_causal = torch.tensor(admittance_matrix, dtype=torch.float32)
        # å½’ä¸€åŒ–
        initial_causal = initial_causal / (initial_causal.max() + 1e-8)
    else:
        # å‡åŒ€åˆå§‹åŒ–
        initial_causal = torch.ones(num_nodes, num_nodes) / num_nodes

    # åº”ç”¨æ©ç 
    initial_causal = initial_causal * adj_mask * direction_mask

    return {
        'adj_mask': adj_mask,
        'direction_mask': direction_mask,
        'initial_causal': initial_causal,
        'node_depths': node_depths.float()
    }


# ============================================================
# ğŸ”¥ ä¼ ç»ŸGCNä»£ç 
# ============================================================
class GCN(torch.nn.Module):
    """
    æ™®é€šå›¾å·ç§¯ç½‘ç»œ - ç”¨äºå•æ—¶é—´æ­¥é¢„æµ‹ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
    """
    def __init__(self, num_features, num_classes):
        super(GCN, self).__init__()
        self.lin_in = torch.nn.Linear(num_features, 64)
        self.conv1 = GCNConv(64, 128)
        self.bn1 = BatchNorm(128)
        self.conv2 = GCNConv(128, 128)
        self.bn2 = BatchNorm(128)
        self.conv3 = GCNConv(128, 64)
        self.bn3 = BatchNorm(64)
        self.lin_out = torch.nn.Linear(64, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        edge_weight = data.edge_attr if hasattr(data, 'edge_attr') and data.edge_attr is not None else None

        x = self.lin_in(x)
        x = F.relu(x)

        x = self.conv1(x, edge_index, edge_weight=edge_weight)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.dropout(x, p=0.3, training=self.training)

        x = self.conv2(x, edge_index, edge_weight=edge_weight)
        x = self.bn2(x)
        x = F.relu(x)
        x = F.dropout(x, p=0.3, training=self.training)

        x = self.conv3(x, edge_index, edge_weight=edge_weight)
        x = self.bn3(x)
        x = F.relu(x)

        x = self.lin_out(x)
        return F.log_softmax(x, dim=1)



# ============================================================
# ğŸ”¥ TGCN: Temporal Graph Convolutional Network
# ============================================================
def batch_static_graph(edge_index, edge_weight, batch_size, num_nodes):
    """
    è¾…åŠ©å‡½æ•°: å°†é™æ€å›¾æ‰©å±•ä¸º batch å›¾ (Block Diagonal)
    [2, E] -> [2, E * B]
    """
    num_edges = edge_index.size(1)
    device = edge_index.device
    
    # ç”Ÿæˆåç§»é‡: [0, N, 2N, ..., (B-1)N]
    shift = torch.arange(batch_size, device=device).view(-1, 1) * num_nodes
    shift = shift.repeat(1, num_edges).view(-1)
    
    # å¤åˆ¶è¾¹ç´¢å¼•å¹¶åŠ ä¸Šåç§»
    batched_edge_index = edge_index.repeat(1, batch_size)  # [2, B*E]
    batched_edge_index += shift
    
    # å¤åˆ¶è¾¹æƒé‡
    batched_edge_weight = edge_weight.repeat(batch_size) if edge_weight is not None else None
    
    return batched_edge_index, batched_edge_weight
class TGCNCell(nn.Module):
    """
    TGCN å•å…ƒ: GCN + GRU çš„èåˆ
    - GCN è´Ÿè´£ç©ºé—´ç‰¹å¾æå– (æ•æ‰èŠ‚ç‚¹é—´ç”µæ°”è€¦åˆå…³ç³»)
    - GRU è´Ÿè´£æ—¶é—´åºåˆ—å»ºæ¨¡ (æ•æ‰è´Ÿè·å˜åŒ–çš„æ—¶åºæ¨¡å¼)
    
    å‚è€ƒè®ºæ–‡: T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction
    """
    def __init__(self, in_channels, out_channels):
        super(TGCNCell, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # GCN ç”¨äºæå–ç©ºé—´ç‰¹å¾ (ç”¨äº GRU çš„ reset/update gate)
        self.graph_conv1 = GCNConv(in_channels + out_channels, out_channels)
        self.graph_conv2 = GCNConv(in_channels + out_channels, out_channels)
        self.graph_conv3 = GCNConv(in_channels + out_channels, out_channels)
        
    def forward(self, x, edge_index, edge_weight, h):
        combined = torch.cat([x, h], dim=1)
        
        # Reset gate
        r = torch.sigmoid(self.graph_conv1(combined, edge_index, edge_weight))
        # Update gate
        u = torch.sigmoid(self.graph_conv2(combined, edge_index, edge_weight))
        # å€™é€‰éšçŠ¶æ€
        combined_r = torch.cat([x, r * h], dim=1)
        c = torch.tanh(self.graph_conv3(combined_r, edge_index, edge_weight))
        # æœ€ç»ˆéšçŠ¶æ€
        h_new = u * h + (1 - u) * c
        
        return h_new
class TGCN(nn.Module):
    """
    æ—¶åºå›¾å·ç§¯ç½‘ç»œ (Batch å¹¶è¡Œç‰ˆ)
    
    æ¶æ„:
    1. è¾“å…¥ç¼–ç : Linear æŠŠåŸå§‹ç‰¹å¾æ˜ å°„åˆ°éšè—ç»´åº¦
    2. TGCN Cell: å¤šä¸ªæ—¶é—´æ­¥å…±äº«åŒä¸€ä¸ª TGCN Cellï¼Œé€æ­¥æ›´æ–°éšçŠ¶æ€
    3. è¾“å‡ºå±‚: ç”¨æœ€åæ—¶åˆ»çš„éšçŠ¶æ€åšåˆ†ç±»
    
    é€‚ç”¨äº: ç”µç½‘è´Ÿè·å¼‚å¸¸æ£€æµ‹ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘ç©ºé—´æ‹“æ‰‘å’Œæ—¶é—´æ¼”å˜
    """
    def __init__(self, num_features, num_classes, hidden_dim=64, num_layers=2):
        super(TGCN, self).__init__()
        
        self.num_features = num_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.lin_in = nn.Linear(num_features, hidden_dim)
        
        self.tgcn_cells = nn.ModuleList()
        for i in range(num_layers):
            self.tgcn_cells.append(TGCNCell(hidden_dim, hidden_dim))
        
        self.bn = BatchNorm(hidden_dim)
        self.lin_out = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x_seq, edge_index, edge_weight, node_mask=None):
        """
        Args:
            x_seq: [Batch, Seq, Nodes, Features] æˆ– [Seq, Nodes, Features]
        """
        # å¤„ç† Batch ç»´åº¦
        if x_seq.dim() == 4:
            batch_size, seq_len, num_nodes, _ = x_seq.shape
            
            # 1. æ‰©å±•å›¾ç»“æ„ (Batch Graph)
            edge_index, edge_weight = batch_static_graph(edge_index, edge_weight, batch_size, num_nodes)
            
            # 2. å˜å½¢æ•°æ®: [Batch, Seq, Nodes, F] -> [Seq, Batch*Nodes, F]
            # è¿™æ · GCN å°±å¯ä»¥æŠŠ (Batch*Nodes) å½“ä½œä¸€ä¸ªè¶…å¤§å›¾çš„æ‰€æœ‰èŠ‚ç‚¹ä¸€æ¬¡æ€§å¤„ç†
            x_seq = x_seq.permute(1, 0, 2, 3).reshape(seq_len, batch_size * num_nodes, -1)
            
            total_nodes = batch_size * num_nodes
        else:
            # å•æ ·æœ¬æƒ…å†µ (ä¿ç•™å…¼å®¹æ€§)
            seq_len, num_nodes, _ = x_seq.shape
            batch_size = 1
            total_nodes = num_nodes
        
        device = x_seq.device
        
        # åˆå§‹åŒ–éšçŠ¶æ€ [Total_Nodes, Hidden]
        h_list = [
            torch.zeros(total_nodes, self.hidden_dim, device=device)
            for _ in range(self.num_layers)
        ]
        
        # é€æ—¶é—´æ­¥å¤„ç† (ä½†å¹¶è¡Œçš„ Batch)
        for t in range(seq_len):
            x_t = x_seq[t]  # [Total_Nodes, Features]
            
            x_t = self.lin_in(x_t)
            x_t = F.relu(x_t)
            
            for layer_idx, cell in enumerate(self.tgcn_cells):
                h_list[layer_idx] = cell(x_t, edge_index, edge_weight, h_list[layer_idx])
                x_t = h_list[layer_idx]
                if layer_idx < self.num_layers - 1:
                    x_t = self.dropout(x_t)
        
        # Output reshape: [Total_Nodes, Hidden]
        h_final = h_list[-1]
        h_final = self.bn(h_final)
        h_final = F.relu(h_final)
        
        out = self.lin_out(h_final) # [Total_Nodes, Classes]
        
        # è¿˜åŸå½¢çŠ¶: [Batch*Nodes, C] -> [Batch, Nodes, C]
        if batch_size > 1:
            out = out.view(batch_size, num_nodes, -1)
            
        return F.log_softmax(out, dim=-1)


# ============================================================
# ğŸ”¥ NGCæ ¼å…°æ°å› æœæ¨¡å‹
# ğŸ”¥ Neural Granger Causality (NGC) - ç¥ç»æ ¼å…°æ°å› æœæ¨¡å‹
# ============================================================
class NeuralGrangerCausality(nn.Module):
    """
    ç¥ç»æ ¼å…°æ°å› æœæ¨¡å‹ - ç”¨äºå‘ç°æ—¶åºæ•°æ®ä¸­çš„å› æœå…³ç³»
    
    åŸç†:
    - ä½¿ç”¨å¯å­¦ä¹ çš„å› æœæƒé‡çŸ©é˜µ Wï¼ŒW[i,j] è¡¨ç¤ºèŠ‚ç‚¹ j å¯¹èŠ‚ç‚¹ i çš„å› æœå½±å“å¼ºåº¦
    - é€šè¿‡ L1 ç¨€ç–æ€§æ­£åˆ™åŒ–ï¼Œè‡ªåŠ¨å‘ç°ç¨€ç–çš„å› æœç»“æ„
    - ç»“åˆ GCN è¿›è¡Œç©ºé—´ç‰¹å¾æå–ï¼ŒGRU è¿›è¡Œæ—¶é—´åºåˆ—å»ºæ¨¡
    
    Loss = é¢„æµ‹Loss + Î» * ||W||_1 (ç¨€ç–æ€§çº¦æŸ)
    
    å‚è€ƒ: Neural Granger Causality (Tank et al., 2021)
    """
    def __init__(self, num_nodes, num_features, num_classes, hidden_dim=64, 
                 num_layers=2, sparsity_lambda=0.01):
        super(NeuralGrangerCausality, self).__init__()
        
        self.num_nodes = num_nodes
        self.num_features = num_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.sparsity_lambda = sparsity_lambda
        
        # ğŸ”¥ æ ¸å¿ƒ: å¯å­¦ä¹ çš„å› æœæƒé‡çŸ©é˜µ W [num_nodes, num_nodes]
        # W[i, j] è¡¨ç¤ºèŠ‚ç‚¹ j å¯¹èŠ‚ç‚¹ i çš„æ ¼å…°æ°å› æœå½±å“
        # åˆå§‹åŒ–ä¸ºå°çš„éšæœºå€¼ï¼Œè®©ç½‘ç»œè‡ªå·±å­¦ä¹ ç¨€ç–ç»“æ„
        self.causal_weight = nn.Parameter(torch.randn(num_nodes, num_nodes) * 0.01)
        
        # è¾“å…¥ç¼–ç å±‚
        self.lin_in = nn.Linear(num_features, hidden_dim)
        
        # å› æœæ³¨æ„åŠ›èåˆå±‚ - å°†å› æœæƒé‡èå…¥ç‰¹å¾
        self.causal_fusion = nn.Linear(hidden_dim * 2, hidden_dim)
        
        # GCN å±‚ç”¨äºç©ºé—´ç‰¹å¾æå–
        self.gcn_layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()
        for i in range(num_layers):
            self.gcn_layers.append(GCNConv(hidden_dim, hidden_dim))
            self.bn_layers.append(BatchNorm(hidden_dim))
        
        # GRU ç”¨äºæ—¶é—´åºåˆ—å»ºæ¨¡
        self.gru = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=False,
            dropout=0.2
        )
        
        # è¾“å‡ºå±‚
        self.bn_out = BatchNorm(hidden_dim)
        self.lin_out = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(0.3)
        
    def get_causal_matrix(self, threshold=0.1):
        """
        è·å–å› æœçŸ©é˜µ (ç»è¿‡é˜ˆå€¼å¤„ç†)
        threshold: å°äºè¯¥å€¼çš„å› æœæƒé‡è¢«è§†ä¸ºæ— å› æœå…³ç³»
        """
        with torch.no_grad():
            W = torch.abs(self.causal_weight)
            # å½’ä¸€åŒ–åˆ° [0, 1]
            W = W / (W.max() + 1e-8)
            # é˜ˆå€¼å¤„ç†
            W = torch.where(W > threshold, W, torch.zeros_like(W))
            return W.cpu().numpy()
    
    def get_sparsity_loss(self):
        """
        è®¡ç®—ç¨€ç–æ€§æ­£åˆ™åŒ– Loss (L1 èŒƒæ•°)
        """
        return self.sparsity_lambda * torch.sum(torch.abs(self.causal_weight))
    
    def apply_causal_attention(self, x):
        """
        åº”ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶
        Args:
            x: [num_nodes, hidden_dim]
        Returns:
            x_causal: [num_nodes, hidden_dim] èåˆäº†å› æœä¿¡æ¯çš„ç‰¹å¾
        """
        # è·å–å› æœæƒé‡çš„ softmax (ä½¿å¾—æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥æƒé‡å’Œä¸º1)
        W = torch.softmax(self.causal_weight, dim=1)  # [N, N]
        
        # é€šè¿‡å› æœæƒé‡èšåˆé‚»å±…ä¿¡æ¯
        # x_agg[i] = sum_j W[i,j] * x[j] 
        x_agg = torch.matmul(W, x)  # [N, hidden]
        
        # èåˆåŸå§‹ç‰¹å¾å’Œå› æœèšåˆç‰¹å¾
        x_concat = torch.cat([x, x_agg], dim=-1)  # [N, 2*hidden]
        x_causal = self.causal_fusion(x_concat)  # [N, hidden]
        
        return F.relu(x_causal)
        
    def forward(self, x_seq, edge_index, edge_weight, node_mask=None):
        """
        Args:
            x_seq: [seq_len, num_nodes, features] æˆ– [batch, seq_len, num_nodes, features]
        Returns:
            out: [num_nodes, num_classes] æˆ– [batch, num_nodes, num_classes]
        """
        # å¤„ç† batch ç»´åº¦
        if x_seq.dim() == 4:
            batch_size, seq_len, num_nodes, _ = x_seq.shape
            # é€æ ·æœ¬å¤„ç† (å› ä¸ºå› æœçŸ©é˜µæ˜¯å›ºå®šçš„)
            outputs = []
            for b in range(batch_size):
                out_b = self._forward_single(x_seq[b], edge_index, edge_weight)
                outputs.append(out_b)
            return torch.stack(outputs, dim=0)  # [B, N, C]
        else:
            return self._forward_single(x_seq, edge_index, edge_weight)
    
    def _forward_single(self, x_seq, edge_index, edge_weight):
        """
        å•æ ·æœ¬å‰å‘ä¼ æ’­
        Args:
            x_seq: [seq_len, num_nodes, features]
        """
        seq_len, num_nodes, _ = x_seq.shape
        
        outputs = []
        for t in range(seq_len):
            x_t = x_seq[t]  # [N, F]
            
            # 1. è¾“å…¥ç¼–ç 
            x_t = self.lin_in(x_t)
            x_t = F.relu(x_t)
            
            # 2. åº”ç”¨å› æœæ³¨æ„åŠ›
            x_t = self.apply_causal_attention(x_t)
            
            # 3. GCN ç©ºé—´ç‰¹å¾æå–
            for i, (gcn, bn) in enumerate(zip(self.gcn_layers, self.bn_layers)):
                x_t = gcn(x_t, edge_index, edge_weight)
                x_t = bn(x_t)
                x_t = F.relu(x_t)
                if i < self.num_layers - 1:
                    x_t = self.dropout(x_t)
            
            outputs.append(x_t)
        
        # 4. GRU æ—¶é—´åºåˆ—å»ºæ¨¡
        gcn_seq = torch.stack(outputs, dim=0)  # [seq_len, N, hidden]
        gru_out, _ = self.gru(gcn_seq)
        h_final = gru_out[-1]  # [N, hidden]
        
        # 5. è¾“å‡ºå±‚
        h_final = self.bn_out(h_final)
        h_final = F.relu(h_final)
        out = self.lin_out(h_final)  # [N, num_classes]
        
        return F.log_softmax(out, dim=-1)


# ============================================================
# ğŸ”¥ æ—¶åºGCN
# ============================================================
class TemporalGCN(nn.Module):
    """
    ç®€åŒ–ç‰ˆæ—¶åºGNN: GCN + GRU (æ›´æ¨¡å—åŒ–ï¼Œæ˜“äºç†è§£)
    """
    def __init__(self, num_features, num_classes, hidden_dim=64, gru_layers=2):
        super(TemporalGCN, self).__init__()
        
        self.hidden_dim = hidden_dim
        
        self.lin_in = nn.Linear(num_features, hidden_dim)
        self.conv1 = GCNConv(hidden_dim, hidden_dim)
        self.bn1 = BatchNorm(hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.bn2 = BatchNorm(hidden_dim)
        
        self.gru = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=gru_layers,
            batch_first=False,
            dropout=0.2 if gru_layers > 1 else 0
        )
        
        self.lin_out = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x_seq, edge_index, edge_weight, node_mask=None):
        """
        Args:
            x_seq: æ—¶é—´åºåˆ— [seq_len, num_nodes, num_features]
        Returns:
            out: [num_nodes, num_classes]
        """
        seq_len, num_nodes, _ = x_seq.shape
        
        gcn_outputs = []
        for t in range(seq_len):
            x_t = x_seq[t]
            x_t = self.lin_in(x_t)
            x_t = F.relu(x_t)
            x_t = self.conv1(x_t, edge_index, edge_weight)
            x_t = self.bn1(x_t)
            x_t = F.relu(x_t)
            x_t = self.dropout(x_t)
            x_t = self.conv2(x_t, edge_index, edge_weight)
            x_t = self.bn2(x_t)
            x_t = F.relu(x_t)
            gcn_outputs.append(x_t)
        
        gcn_seq = torch.stack(gcn_outputs, dim=0)
        gru_out, _ = self.gru(gcn_seq)
        h_final = gru_out[-1]
        
        out = self.lin_out(h_final)
        return F.log_softmax(out, dim=1)



# ============================================================
# ğŸ”¥ å› æœ+lstm  CausalGCN_LSTM
# ============================================================
class CausalAttention(nn.Module):
    """
    å› æœæ³¨æ„åŠ›å±‚

    å­¦ä¹ èŠ‚ç‚¹é—´çš„å› æœå½±å“å¼ºåº¦ï¼Œå—ç‰©ç†çº¦æŸ
    """

    def __init__(self, input_dim: int, num_nodes: int,
                 adj_mask: torch.Tensor,
                 direction_mask: torch.Tensor,
                 initial_causal: Optional[torch.Tensor] = None):
        super().__init__()

        self.num_nodes = num_nodes

        # æ³¨å†Œæ©ç ä¸º bufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        self.register_buffer('adj_mask', adj_mask)
        self.register_buffer('direction_mask', direction_mask)

        # å¯å­¦ä¹ çš„å› æœå¼ºåº¦çŸ©é˜µ
        if initial_causal is not None:
            self.causal_logits = nn.Parameter(initial_causal.clone())
        else:
            self.causal_logits = nn.Parameter(torch.zeros(num_nodes, num_nodes))

        # ç‰¹å¾å˜æ¢
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)

        self.scale = input_dim ** 0.5

    def get_causal_matrix(self) -> torch.Tensor:
        """
        è·å–æœ‰æ•ˆå› æœçŸ©é˜µ = softmax(å­¦ä¹ åˆ°çš„å¼ºåº¦) Ã— ç‰©ç†çº¦æŸ
        """
        # Softmax å½’ä¸€åŒ–
        C = torch.softmax(self.causal_logits, dim=-1)

        # åº”ç”¨ç‰©ç†çº¦æŸ
        C = C * self.adj_mask * self.direction_mask

        # é‡æ–°å½’ä¸€åŒ–
        row_sum = C.sum(dim=-1, keepdim=True)
        row_sum = torch.clamp(row_sum, min=1e-8)
        C = C / row_sum

        return C

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (batch, num_nodes, features)

        Returns:
            output: (batch, num_nodes, features) å› æœèšåˆåçš„ç‰¹å¾
            causal_matrix: (num_nodes, num_nodes) å› æœçŸ©é˜µ
        """
        batch_size = x.size(0)

        # è·å–å› æœçŸ©é˜µ
        C = self.get_causal_matrix()  # (N, N)

        # Query, Key, Value
        Q = self.query(x)  # (B, N, F)
        K = self.key(x)  # (B, N, F)
        V = self.value(x)  # (B, N, F)

        # æ³¨æ„åŠ›åˆ†æ•° = Q @ K^T / sqrt(d)
        attn_scores = torch.bmm(Q, K.transpose(-2, -1)) / self.scale  # (B, N, N)

        # èåˆå­¦ä¹ åˆ°çš„å› æœçŸ©é˜µ
        # å› æœçŸ©é˜µä½œä¸ºå…ˆéªŒï¼Œè°ƒåˆ¶æ³¨æ„åŠ›
        attn_weights = torch.softmax(attn_scores, dim=-1)  # (B, N, N)
        attn_weights = attn_weights * C.unsqueeze(0)  # åº”ç”¨å› æœçº¦æŸ

        # é‡æ–°å½’ä¸€åŒ–
        attn_weights = attn_weights / (attn_weights.sum(dim=-1, keepdim=True) + 1e-8)

        # èšåˆ
        output = torch.bmm(attn_weights, V)  # (B, N, F)

        return output, C


class PhysicsGuidedCausalGCN(nn.Module):
    """
    ç‰©ç†å¼•å¯¼çš„å› æœå›¾å·ç§¯ç½‘ç»œ

    ç‰¹ç‚¹ï¼š
    1. ç”¨å¯¼çº³çŸ©é˜µåˆå§‹åŒ–å› æœå¼ºåº¦
    2. é‚»æ¥æ©ç ï¼šåªæœ‰ç‰©ç†ç›¸é‚»èŠ‚ç‚¹æœ‰å› æœå…³ç³»
    3. æ–¹å‘æ©ç ï¼šå› æœåªèƒ½ä»ä¸Šæ¸¸ä¼ åˆ°ä¸‹æ¸¸
    4. è¾“å‡ºå› æœçŸ©é˜µï¼Œå¯è§£é‡Š
    """

    def __init__(self, num_nodes: int, input_dim: int, hidden_dim: int,
                 edge_index: np.ndarray,
                 admittance_matrix: Optional[np.ndarray] = None,
                 source_node: int = 105,
                 num_layers: int = 2,
                 dropout: float = 0.3):
        super().__init__()

        self.num_nodes = num_nodes
        self.hidden_dim = hidden_dim

        # æ„å»ºå› æœæ©ç 
        masks = build_causal_masks(
            edge_index, num_nodes, source_node, admittance_matrix
        )

        # è¾“å…¥å˜æ¢
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # å› æœæ³¨æ„åŠ›å±‚
        self.causal_attention = CausalAttention(
            hidden_dim, num_nodes,
            masks['adj_mask'],
            masks['direction_mask'],
            masks['initial_causal']
        )

        # GCN å±‚
        self.gcn_layers = nn.ModuleList()
        for i in range(num_layers):
            self.gcn_layers.append(GCNConv(hidden_dim, hidden_dim))

        # èåˆå±‚
        self.fusion = nn.Linear(hidden_dim * 2, hidden_dim)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)

        # ä¿å­˜èŠ‚ç‚¹æ·±åº¦
        self.register_buffer('node_depths', masks['node_depths'])

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (batch, num_nodes, features)
            edge_index: (2, E)

        Returns:
            h: (batch, num_nodes, hidden_dim)
            causal_matrix: (num_nodes, num_nodes)
        """
        batch_size = x.size(0)

        # è¾“å…¥æŠ•å½±
        h = self.input_proj(x)  # (B, N, H)
        h = F.relu(h)

        # å› æœæ³¨æ„åŠ›èšåˆ
        h_causal, causal_matrix = self.causal_attention(h)  # (B, N, H), (N, N)

        # GCN èšåˆï¼ˆé€ batch å¤„ç†ï¼‰
        h_gcn_list = []
        for b in range(batch_size):
            h_b = h[b]  # (N, H)
            for gcn in self.gcn_layers:
                h_b = gcn(h_b, edge_index)
                h_b = F.relu(h_b)
                h_b = self.dropout(h_b)
            h_gcn_list.append(h_b)

        h_gcn = torch.stack(h_gcn_list, dim=0)  # (B, N, H)

        # èåˆå› æœèšåˆå’Œ GCN èšåˆ
        h_combined = torch.cat([h_causal, h_gcn], dim=-1)  # (B, N, 2H)
        h = self.fusion(h_combined)  # (B, N, H)
        h = self.layer_norm(h)

        return h, causal_matrix


class CausalGCN_LSTM(nn.Module):
    """
    å› æœæ„ŸçŸ¥çš„ GCN-LSTM æ¨¡å‹

    æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è¾“å…¥: (batch, seq_len, num_nodes, features)            â”‚
    â”‚                      â†“                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  PhysicsGuidedCausalGCN (æ¯ä¸ªæ—¶é—´æ­¥)             â”‚   â”‚
    â”‚  â”‚  â€¢ å› æœæ³¨æ„åŠ› + GCN èšåˆ                         â”‚   â”‚
    â”‚  â”‚  â€¢ è¾“å‡ºå› æœçŸ©é˜µ                                  â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                      â†“                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  LSTM (æ—¶åºå»ºæ¨¡)                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                      â†“                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  å¼‚å¸¸åˆ†ç±»å¤´       â”‚  æ ¹å› åˆ¤åˆ«å¤´                   â”‚   â”‚
    â”‚  â”‚  (0/1/2/3)       â”‚  (æ˜¯å¦æ˜¯æ•…éšœæº)               â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    def __init__(self, num_nodes: int, num_features: int, num_classes: int,
                 edge_index: np.ndarray,
                 admittance_matrix: Optional[np.ndarray] = None,
                 source_node: int = 105,
                 gcn_hidden: int = 64,
                 lstm_hidden: int = 128,
                 num_gcn_layers: int = 2,
                 num_lstm_layers: int = 2,
                 dropout: float = 0.3):
        super().__init__()

        self.num_nodes = num_nodes
        self.num_classes = num_classes

        # å› æœ GCN
        self.causal_gcn = PhysicsGuidedCausalGCN(
            num_nodes=num_nodes,
            input_dim=num_features,
            hidden_dim=gcn_hidden,
            edge_index=edge_index,
            admittance_matrix=admittance_matrix,
            source_node=source_node,
            num_layers=num_gcn_layers,
            dropout=dropout
        )

        # LSTM æ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=gcn_hidden,
            hidden_size=lstm_hidden,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0,
            bidirectional=True
        )

        lstm_output_dim = lstm_hidden * 2  # åŒå‘

        # ä»»åŠ¡1: å¼‚å¸¸åˆ†ç±»å¤´
        self.anomaly_classifier = nn.Sequential(
            nn.Linear(lstm_output_dim, lstm_output_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_output_dim // 2, num_classes)
        )

        # ä»»åŠ¡2: æ ¹å› åˆ¤åˆ«å¤´
        # ç»“åˆèŠ‚ç‚¹æ·±åº¦ä¿¡æ¯ï¼ˆä¸Šæ¸¸èŠ‚ç‚¹æ›´å¯èƒ½æ˜¯æ ¹å› ï¼‰
        self.root_cause_head = nn.Sequential(
            nn.Linear(lstm_output_dim + 1, lstm_output_dim // 2),  # +1 for depth
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_output_dim // 2, 1)
        )

        # ä¿å­˜è¾¹ç´¢å¼•
        self.register_buffer('edge_index', torch.LongTensor(edge_index))
        self.register_buffer('node_depths', self.causal_gcn.node_depths)

    def forward(self, x: torch.Tensor,
                edge_index: Optional[torch.Tensor] = None,
                return_causal: bool = False) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: (batch, seq_len, num_nodes, num_features)
            edge_index: (2, E) å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„
            return_causal: æ˜¯å¦è¿”å›å› æœçŸ©é˜µ

        Returns:
            dict: {
                'anomaly_logits': (batch, num_nodes, num_classes),
                'root_cause_logits': (batch, num_nodes),
                'causal_matrix': (num_nodes, num_nodes) [if return_causal]
            }
        """
        batch_size, seq_len, N, F = x.shape

        if edge_index is None:
            edge_index = self.edge_index

        # å¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œå› æœ GCN
        h_seq = []
        causal_matrix = None

        for t in range(seq_len):
            x_t = x[:, t, :, :]  # (B, N, F)
            h_t, causal_matrix = self.causal_gcn(x_t, edge_index)  # (B, N, H)
            h_seq.append(h_t)

        # å †å æ—¶é—´ç»´åº¦
        h_seq = torch.stack(h_seq, dim=1)  # (B, T, N, H)

        # LSTM å¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„æ—¶åº
        # é‡æ’ä¸º (B*N, T, H)
        h_seq = h_seq.permute(0, 2, 1, 3).contiguous()  # (B, N, T, H)
        h_seq = h_seq.view(batch_size * N, seq_len, -1)  # (B*N, T, H)

        lstm_out, _ = self.lstm(h_seq)  # (B*N, T, 2*lstm_hidden)

        # å–æœ€åæ—¶é—´æ­¥
        h_final = lstm_out[:, -1, :]  # (B*N, 2*lstm_hidden)
        h_final = h_final.view(batch_size, N, -1)  # (B, N, 2*lstm_hidden)

        # ä»»åŠ¡1: å¼‚å¸¸åˆ†ç±»
        anomaly_logits = self.anomaly_classifier(h_final)  # (B, N, num_classes)

        # ä»»åŠ¡2: æ ¹å› åˆ¤åˆ«
        # æ·»åŠ èŠ‚ç‚¹æ·±åº¦ç‰¹å¾ï¼ˆå½’ä¸€åŒ–ï¼‰
        depth_feat = self.node_depths.unsqueeze(0).expand(batch_size, -1)  # (B, N)
        depth_feat = depth_feat / (depth_feat.max() + 1e-8)  # å½’ä¸€åŒ–
        depth_feat = depth_feat.unsqueeze(-1)  # (B, N, 1)

        h_with_depth = torch.cat([h_final, depth_feat], dim=-1)  # (B, N, 2*lstm_hidden+1)
        root_cause_logits = self.root_cause_head(h_with_depth).squeeze(-1)  # (B, N)

        outputs = {
            'anomaly_logits': anomaly_logits,
            'root_cause_logits': root_cause_logits
        }

        if return_causal:
            outputs['causal_matrix'] = causal_matrix

        return outputs

    def get_causal_matrix(self) -> torch.Tensor:
        """è·å–å½“å‰çš„å› æœçŸ©é˜µ"""
        return self.causal_gcn.causal_attention.get_causal_matrix()

    def causal_sparsity_loss(self) -> torch.Tensor:
        """å› æœç¨€ç–æ€§æŸå¤±"""
        C = self.get_causal_matrix()
        return torch.mean(torch.abs(C))


def create_causal_model(num_nodes: int, num_features: int, num_classes: int,
                        edge_index: np.ndarray,
                        admittance_matrix: Optional[np.ndarray] = None,
                        source_node: int = 105,
                        **kwargs) -> CausalGCN_LSTM:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå› æœ GCN-LSTM æ¨¡å‹
    """
    return CausalGCN_LSTM(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=num_classes,
        edge_index=edge_index,
        admittance_matrix=admittance_matrix,
        source_node=source_node,
        **kwargs
    )
