import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import networkx as nx
import os

# å¯¼å…¥æˆ‘ä»¬åœ¨ src é‡Œå†™çš„æ¨¡å—
from src.dataset import PowerGridDataset
from src.models import GCN, TGCN, TemporalGCN, NeuralGrangerCausality, CausalGCN_LSTM, create_causal_model
from src.loss.causal_loss import CausalLoss, create_causal_loss

# å¯¼å…¥ç”»å›¾å‡½æ•°
from plot.causal_plot import plot_causal_graph, analyze_causal_structure

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# ============================================================
# é…ç½®: é€‰æ‹©æ¨¡å‹ç±»å‹
# ============================================================
USE_TEMPORAL = True  # True: ä½¿ç”¨æ—¶åºGNN (TGCN), False: ä½¿ç”¨æ™®é€šGCN
USE_NGC = False        # True: ä½¿ç”¨ç¥ç»æ ¼å…°æ°å› æœæ¨¡å‹ (ä¼˜å…ˆçº§æœ€é«˜)
USE_CAUSAL_LSTM = False  # True: ä½¿ç”¨å› æœGCN-LSTMæ¨¡å‹ (æœ€é«˜ä¼˜å…ˆçº§)
SEQ_LEN = 12         # æ—¶é—´çª—å£é•¿åº¦ (ä»… TGCN ä½¿ç”¨)
BATCH_SIZE = 32      # æ‰¹å¤§å°
SPARSITY_LAMBDA = 0.01  # ç¨€ç–æ€§æ­£åˆ™åŒ–ç³»æ•° (NGC ä½¿ç”¨)


def train_causal_gcn_lstm():
    """
    ä½¿ç”¨ CausalGCN_LSTM å› æœæ„ŸçŸ¥æ¨¡å‹è®­ç»ƒ
    
    ç‰¹ç‚¹:
    1. ç‰©ç†å¼•å¯¼çš„å› æœæ³¨æ„åŠ›æœºåˆ¶
    2. ä½¿ç”¨å› æœæŸå¤±å‡½æ•° (åˆ†ç±»æŸå¤± + æ ¹å› æŸå¤± + ç¨€ç–æ€§æŸå¤± + ç‰©ç†ä¸€è‡´æ€§æŸå¤±)
    3. æ”¯æŒå¼‚å¸¸åˆ†ç±»å’Œæ ¹å› åˆ¤åˆ«åŒä»»åŠ¡
    4. è®­ç»ƒå®Œæˆåç»˜åˆ¶å› æœå›¾
    """
    print("=" * 60)
    print("ğŸ§  ä½¿ç”¨ CausalGCN_LSTM (å› æœæ„ŸçŸ¥ GCN-LSTM) è®­ç»ƒ")
    print("=" * 60)
    
    # 1. å‡†å¤‡æ•°æ®
    print(">>> [1/6] åŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    
    # è·å–æ—¶åºæ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=SEQ_LEN)
    # X: [num_samples, seq_len, num_nodes, features]
    # Y: [num_samples, num_nodes]
    
    num_samples = X.shape[0]
    num_nodes = X.shape[2]
    num_features = X.shape[3]
    
    print(f">>> æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f">>> èŠ‚ç‚¹æ•°: {num_nodes}, ç‰¹å¾æ•°: {num_features}")
    
    # 2. åˆå§‹åŒ–è®¾å¤‡
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')
    if use_cuda:
        print(f">>> æ£€æµ‹åˆ° CUDA: {torch.version.cuda} | GPU: {torch.cuda.get_device_name(0)}")
    else:
        print(">>> æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # ç§»åŠ¨å›¾ç»“æ„åˆ°è®¾å¤‡
    edge_index_tensor = edge_index.to(device)
    edge_weight_tensor = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # 3. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    
    X_train, Y_train = X[train_idx], Y[train_idx]
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    print(f">>> è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_idx)} æ ·æœ¬")
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    print(f">>> [2/6] åˆå§‹åŒ– CausalGCN_LSTM æ¨¡å‹ (Device: {device})...")
    
    # å°† edge_index è½¬æ¢ä¸º numpy ç”¨äºæ¨¡å‹æ„å»º
    edge_index_np = edge_index.numpy()
    
    model = CausalGCN_LSTM(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=4,
        edge_index=edge_index_np,
        admittance_matrix=None,  # å¦‚æœæœ‰å¯¼çº³çŸ©é˜µå¯ä»¥ä¼ å…¥
        source_node=0,  # å˜å‹å™¨èŠ‚ç‚¹ï¼ˆæºèŠ‚ç‚¹ï¼‰
        gcn_hidden=64,
        lstm_hidden=128,
        num_gcn_layers=2,
        num_lstm_layers=2,
        dropout=0.3
    ).to(device)
    
    # 5. åˆå§‹åŒ–å› æœæŸå¤±å‡½æ•°
    print(">>> [3/6] åˆå§‹åŒ–å› æœæŸå¤±å‡½æ•°...")
    
    # ç±»åˆ«æƒé‡ (å¤„ç†ä¸å¹³è¡¡)
    class_weights = torch.tensor([2.0, 50.0, 50.0, 20.0]).to(device)
    
    # è·å–èŠ‚ç‚¹æ·±åº¦ç”¨äºç‰©ç†æŸå¤±
    node_depths = model.node_depths.clone()
    
    # åˆ›å»ºå› æœæŸå¤±
    criterion = create_causal_loss(
        class_weights=class_weights,
        node_depths=node_depths,
        use_focal_loss=True,
        focal_gamma=2.0,
        lambda_root=0.5,  # æ ¹å› æŸå¤±æƒé‡
        lambda_sparse=0.01,  # ç¨€ç–æ€§æŸå¤±æƒé‡
        lambda_physics=0.1  # ç‰©ç†ä¸€è‡´æ€§æŸå¤±æƒé‡
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
    
    # 6. è®­ç»ƒå¾ªç¯
    print(">>> [4/6] å¼€å§‹è®­ç»ƒ...")
    loss_history = {'total': [], 'anomaly': [], 'sparse': []}
    num_epochs = 100
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        total_anomaly_loss = 0
        total_sparse_loss = 0
        
        # Mini-batch è®­ç»ƒ
        perm = torch.randperm(len(train_idx))
        for i in range(0, len(train_idx), BATCH_SIZE):
            batch_indices = perm[i:i + BATCH_SIZE]
            current_batch_size = len(batch_indices)
            
            # è·å– batch æ•°æ®
            x_batch = X_train[batch_indices].to(device, non_blocking=True)
            y_batch = Y_train[batch_indices].to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(x_batch, return_causal=True)
            
            # è®¡ç®—å› æœæŸå¤±
            loss, loss_dict = criterion(
                model_outputs=outputs,
                anomaly_labels=y_batch,
                root_cause_labels=None,  # å¦‚æœæœ‰æ ¹å› æ ‡ç­¾å¯ä»¥ä¼ å…¥
                x=x_batch,
                model=model
            )
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss_dict.get('total', loss.item())
            total_anomaly_loss += loss_dict.get('anomaly', 0)
            total_sparse_loss += loss_dict.get('sparse', 0)
        
        scheduler.step()
        
        num_batches = len(train_idx) // BATCH_SIZE + 1
        avg_loss = total_loss / num_batches
        avg_anomaly = total_anomaly_loss / num_batches
        avg_sparse = total_sparse_loss / num_batches
        
        loss_history['total'].append(avg_loss)
        loss_history['anomaly'].append(avg_anomaly)
        loss_history['sparse'].append(avg_sparse)
        
        if epoch % 10 == 0:
            # è®¡ç®—å½“å‰å› æœçŸ©é˜µçš„ç¨€ç–åº¦
            causal_matrix = model.get_causal_matrix().detach().cpu().numpy()
            sparsity_ratio = np.sum(causal_matrix > 0.1) / (num_nodes * num_nodes)
            
            print(f"    Epoch {epoch:03d} | Total Loss: {avg_loss:.4f} | "
                  f"Anomaly: {avg_anomaly:.4f} | Sparse: {avg_sparse:.4f} | "
                  f"Matrix Sparsity: {sparsity_ratio*100:.1f}%")
    
    # 7. è¯„ä¼°
    print(">>> [5/6] è¯„ä¼°æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0
    class_correct = [0, 0, 0, 0]
    class_total = [0, 0, 0, 0]
    
    test_batch_size = BATCH_SIZE * 2
    with torch.no_grad():
        for i in range(0, len(test_idx), test_batch_size):
            end_idx = min(i + test_batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            outputs = model(x_batch)
            anomaly_logits = outputs['anomaly_logits']  # [B, N, C]
            pred = anomaly_logits.argmax(dim=2)  # [B, N]
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            valid_mask = mask_expanded
            correct += (pred_flat[valid_mask] == y_flat[valid_mask]).sum().item()
            total += valid_mask.sum().item()
            
            for c in range(4):
                c_mask = valid_mask & (y_flat == c)
                class_total[c] += c_mask.sum().item()
                class_correct[c] += (pred_flat[c_mask] == y_flat[c_mask]).sum().item()
    
    acc = correct / total if total > 0 else 0
    print("=" * 50)
    print(f"âœ… æœ€ç»ˆæµ‹è¯•é›†æ€»å‡†ç¡®ç‡: {acc * 100:.2f}%")
    print(f"   - Class 0 (æ­£å¸¸): {class_correct[0]}/{class_total[0]} ({100*class_correct[0]/max(class_total[0],1):.1f}%)")
    print(f"   - Class 1 (çªå¢): {class_correct[1]}/{class_total[1]} ({100*class_correct[1]/max(class_total[1],1):.1f}%)")
    print(f"   - Class 2 (ä¸¢å¤±): {class_correct[2]}/{class_total[2]} ({100*class_correct[2]/max(class_total[2],1):.1f}%)")
    print(f"   - Class 3 (æ— åŠŸ): {class_correct[3]}/{class_total[3]} ({100*class_correct[3]/max(class_total[3],1):.1f}%)")
    print("=" * 50)
    
    # ä¿å­˜æ¨¡å‹
    save_dir = "result/causal_gcn_lstm"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    torch.save(model.state_dict(), f"{save_dir}/model.pth")
    checkpoint = {
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
        'accuracy': acc,
        'seq_len': SEQ_LEN,
        'num_nodes': num_nodes,
        'num_features': num_features,
        'num_classes': 4,
        'config': {
            'gcn_hidden': 64,
            'lstm_hidden': 128,
            'num_gcn_layers': 2,
            'num_lstm_layers': 2,
        }
    }
    torch.save(checkpoint, f"{save_dir}/checkpoint.pth")
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}/")
    print(f"   - æƒé‡æ–‡ä»¶: model.pth")
    print(f"   - å®Œæ•´æ£€æŸ¥ç‚¹: checkpoint.pth")
    
    # 8. ç»˜åˆ¶å› æœå›¾
    print("\n>>> [6/6] ç»˜åˆ¶å› æœå›¾...")
    causal_matrix = model.get_causal_matrix().detach().cpu().numpy()
    
    # ä¿å­˜å› æœçŸ©é˜µ
    np.save(f"{save_dir}/causal_matrix.npy", causal_matrix)
    print(f"   å› æœçŸ©é˜µå·²ä¿å­˜: {save_dir}/causal_matrix.npy")
    
    # åˆ†æå› æœç»“æ„
    analyze_causal_structure(causal_matrix, top_k=10)
    
    # ç»˜åˆ¶å› æœå›¾
    plot_causal_graph(causal_matrix, threshold=0.1, save_path=f"{save_dir}/causal_graph.png")
    
    # ç»˜åˆ¶ Loss æ›²çº¿
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    axes[0].plot(loss_history['total'], label='Total Loss', color='blue')
    axes[0].set_title("CausalGCN-LSTM Total Loss")
    axes[0].set_xlabel("Epoch")
    axes[0].set_ylabel("Loss")
    axes[0].legend()
    axes[0].grid(True)
    
    axes[1].plot(loss_history['anomaly'], label='Anomaly Loss', color='green')
    axes[1].set_title("Anomaly Classification Loss")
    axes[1].set_xlabel("Epoch")
    axes[1].set_ylabel("Loss")
    axes[1].legend()
    axes[1].grid(True)
    
    axes[2].plot(loss_history['sparse'], label='Sparsity Loss', color='orange')
    axes[2].set_title("Causal Sparsity Loss")
    axes[2].set_xlabel("Epoch")
    axes[2].set_ylabel("Loss")
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/training_loss.png", dpi=300)
    plt.show()
    
    return model, acc


def train_ngc():
    """
    ä½¿ç”¨ç¥ç»æ ¼å…°æ°å› æœæ¨¡å‹ (NGC) è®­ç»ƒ
    
    ç‰¹ç‚¹:
    1. å¯å­¦ä¹ çš„å› æœæƒé‡çŸ©é˜µ
    2. L1 ç¨€ç–æ€§æ­£åˆ™åŒ–
    3. è®­ç»ƒå®Œæˆåç»˜åˆ¶å› æœå›¾
    """
    print("=" * 50)
    print("ğŸ§  ä½¿ç”¨ Neural Granger Causality (ç¥ç»æ ¼å…°æ°å› æœ) è®­ç»ƒ")
    print("=" * 50)
    
    # 1. å‡†å¤‡æ•°æ®
    print(">>> [1/5] åŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    
    # è·å–æ—¶åºæ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=SEQ_LEN)
    # X: [num_samples, seq_len, num_nodes, features]
    # Y: [num_samples, num_nodes]
    
    num_samples = X.shape[0]
    num_nodes = X.shape[2]
    num_features = X.shape[3]
    
    print(f">>> æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f">>> èŠ‚ç‚¹æ•°: {num_nodes}, ç‰¹å¾æ•°: {num_features}")
    
    # 2. åˆå§‹åŒ–è®¾å¤‡
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')
    if use_cuda:
        print(f">>> æ£€æµ‹åˆ° CUDA: {torch.version.cuda} | GPU: {torch.cuda.get_device_name(0)}")
    else:
        print(">>> æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # ç§»åŠ¨å›¾ç»“æ„åˆ° GPU
    edge_index = edge_index.to(device)
    edge_weight = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # 3. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    
    X_train, Y_train = X[train_idx], Y[train_idx]
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    print(f">>> è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_idx)} æ ·æœ¬")
    
    # 4. åˆå§‹åŒ– NGC æ¨¡å‹
    print(f">>> [2/5] åˆå§‹åŒ– NGC æ¨¡å‹ (Device: {device})...")
    model = NeuralGrangerCausality(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=4,
        hidden_dim=64,
        num_layers=2,
        sparsity_lambda=SPARSITY_LAMBDA
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)
    class_weights = torch.tensor([2.0, 50.0, 50.0, 20.0]).to(device)
    
    print(f">>> ç¨€ç–æ€§æ­£åˆ™åŒ–ç³»æ•° Î» = {SPARSITY_LAMBDA}")
    
    # 5. è®­ç»ƒå¾ªç¯
    print(">>> [3/5] å¼€å§‹è®­ç»ƒ...")
    loss_history = []
    sparsity_loss_history = []
    num_epochs = 100
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        total_sparsity_loss = 0
        
        # Mini-batch è®­ç»ƒ
        perm = torch.randperm(len(train_idx))
        for i in range(0, len(train_idx), BATCH_SIZE):
            batch_indices = perm[i:i + BATCH_SIZE]
            current_batch_size = len(batch_indices)
            
            # è·å– batch æ•°æ®
            x_batch = X_train[batch_indices].to(device, non_blocking=True)
            y_batch = Y_train[batch_indices].to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            out = model(x_batch, edge_index, edge_weight)  # [B, nodes, classes]
            
            # è®¡ç®—åˆ†ç±» Loss
            out_flat = out.view(-1, 4)  # [B*N, 4]
            y_flat = y_batch.view(-1)   # [B*N]
            mask_expanded = node_mask.repeat(current_batch_size)
            
            cls_loss = F.nll_loss(out_flat[mask_expanded], y_flat[mask_expanded], weight=class_weights)
            
            # ğŸ”¥ è®¡ç®—ç¨€ç–æ€§æ­£åˆ™åŒ– Loss (æ ¼å…°æ°å› æœçš„æ ¸å¿ƒ)
            sparsity_loss = model.get_sparsity_loss()
            
            # æ€» Loss = åˆ†ç±»Loss + ç¨€ç–æ€§Loss
            loss = cls_loss + sparsity_loss
            
            loss.backward()
            optimizer.step()
            
            total_loss += cls_loss.item()
            total_sparsity_loss += sparsity_loss.item()
        
        avg_loss = total_loss / (len(train_idx) // BATCH_SIZE + 1)
        avg_sparsity = total_sparsity_loss / (len(train_idx) // BATCH_SIZE + 1)
        loss_history.append(avg_loss)
        sparsity_loss_history.append(avg_sparsity)
        
        if epoch % 10 == 0:
            # è®¡ç®—å½“å‰å› æœçŸ©é˜µçš„ç¨€ç–åº¦
            causal_matrix = model.get_causal_matrix(threshold=0.1)
            sparsity_ratio = np.sum(causal_matrix > 0) / (num_nodes * num_nodes)
            print(f"    Epoch {epoch:03d} | Cls Loss: {avg_loss:.4f} | Sparsity Loss: {avg_sparsity:.4f} | Matrix Sparsity: {sparsity_ratio*100:.1f}%")
    
    # 6. è¯„ä¼°
    print(">>> [4/5] è¯„ä¼°æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0
    class_correct = [0, 0, 0, 0]
    class_total = [0, 0, 0, 0]
    
    test_batch_size = BATCH_SIZE * 2
    with torch.no_grad():
        for i in range(0, len(test_idx), test_batch_size):
            end_idx = min(i + test_batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            out = model(x_batch, edge_index, edge_weight)
            pred = out.argmax(dim=2)
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            valid_mask = mask_expanded
            correct += (pred_flat[valid_mask] == y_flat[valid_mask]).sum().item()
            total += valid_mask.sum().item()
            
            for c in range(4):
                c_mask = valid_mask & (y_flat == c)
                class_total[c] += c_mask.sum().item()
                class_correct[c] += (pred_flat[c_mask] == y_flat[c_mask]).sum().item()
    
    acc = correct / total
    print("=" * 40)
    print(f"âœ… æœ€ç»ˆæµ‹è¯•é›†æ€»å‡†ç¡®ç‡: {acc * 100:.2f}%")
    print(f"   - Class 0 (æ­£å¸¸): {class_correct[0]}/{class_total[0]}")
    print(f"   - Class 1 (çªå¢): {class_correct[1]}/{class_total[1]}")
    print(f"   - Class 2 (ä¸¢å¤±): {class_correct[2]}/{class_total[2]}")
    print(f"   - Class 3 (æ— åŠŸ): {class_correct[3]}/{class_total[3]}")
    print("=" * 40)
    
    # ä¿å­˜æ¨¡å‹
    save_dir = "result/ngc"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    torch.save(model.state_dict(), f"{save_dir}/model.pth")
    checkpoint = {
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
        'accuracy': acc,
        'seq_len': SEQ_LEN,
        'num_nodes': num_nodes,
        'num_features': num_features,
        'num_classes': 4,
        'sparsity_lambda': SPARSITY_LAMBDA
    }
    torch.save(checkpoint, f"{save_dir}/checkpoint.pth")
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}/")
    print(f"   - æƒé‡æ–‡ä»¶: model.pth")
    print(f"   - å®Œæ•´æ£€æŸ¥ç‚¹: checkpoint.pth")
    
    # 7. ç»˜åˆ¶å› æœå›¾
    print("\n>>> [5/5] ç»˜åˆ¶æ ¼å…°æ°å› æœå›¾...")
    causal_matrix = model.get_causal_matrix(threshold=0.05)
    
    # ä¿å­˜å› æœçŸ©é˜µ
    np.save(f"{save_dir}/causal_matrix.npy", causal_matrix)
    print(f"   å› æœçŸ©é˜µå·²ä¿å­˜: {save_dir}/causal_matrix.npy")
    
    # åˆ†æå› æœç»“æ„
    analyze_causal_structure(causal_matrix, top_k=10)
    
    # ç»˜åˆ¶å› æœå›¾
    plot_causal_graph(causal_matrix, threshold=0.1, save_path=f"{save_dir}/causal_graph.png")
    
    # ç»˜åˆ¶ Loss æ›²çº¿
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    axes[0].plot(loss_history, label='Classification Loss')
    axes[0].set_title("NGC Training Loss Curve")
    axes[0].set_xlabel("Epoch")
    axes[0].set_ylabel("Loss")
    axes[0].legend()
    axes[0].grid(True)
    
    axes[1].plot(sparsity_loss_history, label='Sparsity Loss', color='orange')
    axes[1].set_title("Sparsity Regularization Loss")
    axes[1].set_xlabel("Epoch")
    axes[1].set_ylabel("L1 Loss")
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/training_loss.png", dpi=300)
    plt.show()


def train_temporal():
    """ä½¿ç”¨æ—¶åºGNN (TGCN) è®­ç»ƒ"""
    print("=" * 50)
    print("ğŸ”¥ ä½¿ç”¨ TGCN (æ—¶åºå›¾å·ç§¯ç½‘ç»œ) è®­ç»ƒ")
    print("=" * 50)
    
    # 1. å‡†å¤‡æ•°æ®
    print(">>> [1/4] åŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    
    # è·å–æ—¶åºæ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=SEQ_LEN)
    # X: [num_samples, seq_len, num_nodes, features]
    # Y: [num_samples, num_nodes]
    
    num_samples = X.shape[0]
    num_features = X.shape[3]
    
    # 2. åˆå§‹åŒ–è®¾å¤‡
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')
    if use_cuda:
        print(f">>> æ£€æµ‹åˆ° CUDA: {torch.version.cuda} | GPU: {torch.cuda.get_device_name(0)}")
    else:
        print(">>> æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # ç§»åŠ¨å›¾ç»“æ„åˆ° GPU
    edge_index = edge_index.to(device)
    edge_weight = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # 3. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    
    X_train, Y_train = X[train_idx], Y[train_idx]
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    print(f">>> è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_idx)} æ ·æœ¬")
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    print(f">>> [2/4] åˆå§‹åŒ– TGCN æ¨¡å‹ (Device: {device})...")
    model = TGCN(
        num_features=num_features,
        num_classes=4,
        hidden_dim=64,
        num_layers=2
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)
    class_weights = torch.tensor([2.0, 50.0, 50.0, 20.0]).to(device)
    
    # 5. è®­ç»ƒå¾ªç¯
    print(">>> [3/4] å¼€å§‹è®­ç»ƒ...")
    loss_history = []
    num_epochs = 100
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        # Mini-batch è®­ç»ƒ
        perm = torch.randperm(len(train_idx))
        for i in range(0, len(train_idx), BATCH_SIZE):
            batch_indices = perm[i:i + BATCH_SIZE]
            current_batch_size = len(batch_indices)
            
            # è·å– batch æ•°æ®
            x_batch = X_train[batch_indices].to(device, non_blocking=True)  # [B, seq_len, nodes, features]
            y_batch = Y_train[batch_indices].to(device, non_blocking=True)  # [B, nodes]
            
            optimizer.zero_grad()
            
            # --- ğŸ”¥ å¹¶è¡Œ Batch å¤„ç† ---
            # ç›´æ¥å°†æ•´ä¸ª batch ä¼ ç»™æ¨¡å‹ï¼Œä¸å†æŒ‰æ ·æœ¬å¾ªç¯
            out = model(x_batch, edge_index, edge_weight)  # [B, nodes, classes]
            
            # è®¡ç®— Loss (éœ€è¦å±•å¹³)
            out_flat = out.view(-1, 4)  # [B*N, 4]
            y_flat = y_batch.view(-1)   # [B*N]
            
            # æ‰©å±• mask: [nodes] -> [B*nodes]
            # æ³¨æ„: æ‰€æœ‰æ ·æœ¬å…±äº«ç›¸åŒçš„æ‹“æ‰‘å’Œ mask
            mask_expanded = node_mask.repeat(current_batch_size)
            
            loss = F.nll_loss(out_flat[mask_expanded], y_flat[mask_expanded], weight=class_weights)
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / (len(train_idx) // BATCH_SIZE + 1)
        loss_history.append(avg_loss)
        
        if epoch % 10 == 0:
            print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f}")
    
    # 6. è¯„ä¼°
    print(">>> [4/4] è¯„ä¼°æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0
    class_correct = [0, 0, 0, 0]
    class_total = [0, 0, 0, 0]
    
    # è¯„ä¼°æ—¶ä¹Ÿä½¿ç”¨ Batch å¤„ç†ä»¥åŠ é€Ÿ
    test_batch_size = BATCH_SIZE * 2
    with torch.no_grad():
        for i in range(0, len(test_idx), test_batch_size):
            end_idx = min(i + test_batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            out = model(x_batch, edge_index, edge_weight) # [B, N, C]
            pred = out.argmax(dim=2) # [B, N]
            
            mask_expanded = node_mask.repeat(current_batch_size) # [B*N]
            
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            # ç»Ÿè®¡
            valid_mask = mask_expanded
            correct += (pred_flat[valid_mask] == y_flat[valid_mask]).sum().item()
            total += valid_mask.sum().item()
            
            for c in range(4):
                c_mask = valid_mask & (y_flat == c)
                class_total[c] += c_mask.sum().item()
                class_correct[c] += (pred_flat[c_mask] == y_flat[c_mask]).sum().item()
    
    acc = correct / total
    print("=" * 40)
    print(f"âœ… æœ€ç»ˆæµ‹è¯•é›†æ€»å‡†ç¡®ç‡: {acc * 100:.2f}%")
    print(f"   - Class 0 (æ­£å¸¸): {class_correct[0]}/{class_total[0]}")
    print(f"   - Class 1 (çªå¢): {class_correct[1]}/{class_total[1]}")
    print(f"   - Class 2 (ä¸¢å¤±): {class_correct[2]}/{class_total[2]}")
    print(f"   - Class 3 (æ— åŠŸ): {class_correct[3]}/{class_total[3]}")
    print("=" * 40)
    
    # ä¿å­˜æ¨¡å‹
    save_dir = "result/tgcn"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    torch.save(model.state_dict(), f"{save_dir}/model.pth")
    checkpoint = {
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
        'accuracy': acc,
        'seq_len': SEQ_LEN,
        'num_features': num_features,
        'num_classes': 4
    }
    torch.save(checkpoint, f"{save_dir}/checkpoint.pth")
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}/")
    print(f"   - æƒé‡æ–‡ä»¶: model.pth")
    print(f"   - å®Œæ•´æ£€æŸ¥ç‚¹: checkpoint.pth")
    
    # ç”»å›¾
    plt.plot(loss_history)
    plt.title("TGCN Training Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.savefig(f"{save_dir}/training_loss.png", dpi=300)
    plt.show()


def train_gcn():
    """ä½¿ç”¨æ™®é€š GCN è®­ç»ƒ (ä¿ç•™åŸæœ‰é€»è¾‘)"""
    from torch_geometric.data import DataLoader
    
    print("=" * 50)
    print("ä½¿ç”¨æ™®é€š GCN è®­ç»ƒ")
    print("=" * 50)
    # 1. å‡†å¤‡æ•°æ®
    print(">>> [1/4] åŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    data_list = dataset.get_pyg_data_list()

    # 2. åˆå§‹åŒ–è®¾å¤‡ (GPU ä¼˜å…ˆ)
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')
    if use_cuda:
        print(
            f">>> æ£€æµ‹åˆ° CUDA: {torch.version.cuda} | GPU: {torch.cuda.get_device_name(0)}"
        )
    else:
        print(">>> æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆå¦‚éœ€ GPUï¼Œè¯·å®‰è£… CUDA ç‰ˆ PyTorchï¼‰")

    # 2. åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
    train_data, test_data = train_test_split(data_list, test_size=0.2, shuffle=False)

    # DataLoader å°ä¼˜åŒ–ï¼šGPU æ—¶å¯ç”¨ pin_memoryï¼Œæ•°æ®æ‹·è´æ›´å¿«
    train_loader = DataLoader(train_data, batch_size=16, shuffle=True, pin_memory=use_cuda, num_workers=0)
    test_loader = DataLoader(test_data, batch_size=16, shuffle=False, pin_memory=use_cuda, num_workers=0)

    # 3. åˆå§‹åŒ–æ¨¡å‹
    print(f">>> [2/4] åˆå§‹åŒ– GCN æ¨¡å‹ (Device: {device})...")

    # è¾“å…¥ç‰¹å¾=3 (P,Q,V)ï¼Œè¾“å‡ºç±»åˆ«=4
    model = GCN(num_features=6, num_classes=4).to(device)

    # ã€ä¿®æ”¹ç‚¹ 1ã€‘ï¼šå­¦ä¹ ç‡ä» 0.01 æ”¹ä¸º 0.005ï¼Œå› ä¸ºåŠ æƒå Loss ä¼šå˜å¤§ï¼Œæ­¥é•¿å°ä¸€ç‚¹æ›´ç¨³
    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

    # ã€ä¿®æ”¹ç‚¹ 2ã€‘ï¼šå®šä¹‰æš´åŠ›æƒé‡ (Violent Weights)
    # å‘Šè¯‰æ¨¡å‹ï¼šçŒœé”™ä¸€ä¸ª Class 2 (ä¸¢å¤±)ï¼Œç›¸å½“äºçŒœé”™ 300 ä¸ªæ­£å¸¸æ ·æœ¬ï¼
    # é¡ºåºå¯¹åº”: [Class 0, Class 1, Class 2, Class 3]
    class_weights = torch.tensor([2.0, 50.0, 50.0, 20.0]).to(device)

    # 4. å¼€å§‹è®­ç»ƒå¾ªç¯
    print(">>> [3/4] å¼€å§‹è®­ç»ƒ...")
    loss_history = []

    model.train()
    for epoch in range(100):  # è®­ç»ƒ 300 è½®
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device, non_blocking=use_cuda)
            optimizer.zero_grad()

            out = model(batch)

            # ã€ä¿®æ”¹ç‚¹ 3ã€‘ï¼šæŠŠ class_weights ä¼ è¿›å»
            # å…³é”®ï¼šåªè®¡ç®— mask=True çš„èŠ‚ç‚¹çš„ Lossï¼Œå¹¶åº”ç”¨æƒé‡
            loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask], weight=class_weights)

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        loss_history.append(avg_loss)

        if epoch % 10 == 0:
            print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f}")

    # 5. è¯„ä¼°ä¸ä¿å­˜
    print(">>> [4/4] è¯„ä¼°æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0

    # æ–°å¢ï¼šé¡ºä¾¿ç»Ÿè®¡ä¸€ä¸‹å„ç±»çš„å‡†ç¡®ç‡ï¼Œé˜²æ­¢åªçœ‹æ€»åˆ†
    class_correct = [0, 0, 0, 0]
    class_total = [0, 0, 0, 0]

    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device, non_blocking=use_cuda)
            pred = model(batch).argmax(dim=1)

            # åªç»Ÿè®¡æœ‰æ•ˆèŠ‚ç‚¹
            mask = batch.train_mask

            # æ€»å‡†ç¡®ç‡
            correct += (pred[mask] == batch.y[mask]).sum().item()
            total += mask.sum().item()

            # åˆ†ç±»ç»Ÿè®¡ (å¯é€‰ï¼Œæ–¹ä¾¿è°ƒè¯•)
            for c in range(4):
                # æ‰¾å‡º mask=True ä¸” label=c çš„èŠ‚ç‚¹
                c_mask = mask & (batch.y == c)
                class_total[c] += c_mask.sum().item()
                class_correct[c] += (pred[c_mask] == batch.y[c_mask]).sum().item()

    acc = correct / total
    print("=" * 40)
    print(f"âœ… æœ€ç»ˆæµ‹è¯•é›†æ€»å‡†ç¡®ç‡: {acc * 100:.2f}%")
    print(f"   - Class 0 (æ­£å¸¸): {class_correct[0]}/{class_total[0]}")
    print(f"   - Class 1 (çªå¢): {class_correct[1]}/{class_total[1]}")
    print(f"   - Class 2 (ä¸¢å¤±): {class_correct[2]}/{class_total[2]}")
    print(f"   - Class 3 (æ— åŠŸ): {class_correct[3]}/{class_total[3]}")
    print("=" * 40)

    # ä¿å­˜æ¨¡å‹ (å®Œæ•´ç‰ˆï¼ŒåŒ…å«æ›´å¤šä¿¡æ¯æ–¹ä¾¿åç»­ä½¿ç”¨)
    save_dir = "result/gcn"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # æ–¹å¼1: åªä¿å­˜æƒé‡ (è½»é‡çº§ï¼Œç”¨äºæ¨ç†)
    torch.save(model.state_dict(), f"{save_dir}/model.pth")
    
    # æ–¹å¼2: ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ (ç”¨äºæ–­ç‚¹ç»­è®­)
    checkpoint = {
        'epoch': 100,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
        'accuracy': acc,
        'class_weights': class_weights.cpu(),
        'num_features': 6,
        'num_classes': 4
    }
    torch.save(checkpoint, f"{save_dir}/checkpoint.pth")
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}/")
    print(f"   - æƒé‡æ–‡ä»¶: model.pth")
    print(f"   - å®Œæ•´æ£€æŸ¥ç‚¹: checkpoint.pth")

    # ç”»å›¾
    plt.plot(loss_history)
    plt.title("Training Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.savefig(f"{save_dir}/training_loss.png", dpi=300)
    plt.show()


if __name__ == "__main__":
    if USE_CAUSAL_LSTM:
        train_causal_gcn_lstm()
    elif USE_NGC:
        train_ngc()
    elif USE_TEMPORAL:
        train_temporal()
    else:
        train_gcn()