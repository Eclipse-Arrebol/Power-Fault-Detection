"""
æ¨¡å‹è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
åŒ…å« Accuracy, Precision, Recall, F1-Score ç­‰æŒ‡æ ‡çš„è®¡ç®—
"""
import torch
import numpy as np
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


class ModelEvaluator:
    """
    æ¨¡å‹è¯„ä¼°å™¨
    è®¡ç®—åˆ†ç±»ä»»åŠ¡çš„å„é¡¹æ€§èƒ½æŒ‡æ ‡
    """
    
    def __init__(self, class_names=None):
        """
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨äºæ˜¾ç¤º
        """
        if class_names is None:
            self.class_names = ['æ­£å¸¸', 'çªå¢', 'ä¸¢å¤±', 'æ— åŠŸ']
        else:
            self.class_names = class_names
        
        self.num_classes = len(self.class_names)
    
    def calculate_metrics(self, y_true, y_pred):
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        
        Args:
            y_true: çœŸå®æ ‡ç­¾ (numpy array or tensor)
            y_pred: é¢„æµ‹æ ‡ç­¾ (numpy array or tensor)
            
        Returns:
            metrics: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        # è½¬æ¢ä¸º numpy array
        if torch.is_tensor(y_true):
            y_true = y_true.cpu().numpy()
        if torch.is_tensor(y_pred):
            y_pred = y_pred.cpu().numpy()
        
        # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ (macro average)
        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)
        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡ (weighted average)
        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        metrics = {
            'accuracy': accuracy,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'f1_macro': f1_macro,
            'precision_weighted': precision_weighted,
            'recall_weighted': recall_weighted,
            'f1_weighted': f1_weighted,
            'precision_per_class': precision_per_class,
            'recall_per_class': recall_per_class,
            'f1_per_class': f1_per_class,
            'confusion_matrix': cm
        }
        
        return metrics
    
    def print_metrics(self, metrics, model_name="Model"):
        """
        æ‰“å°è¯„ä¼°æŒ‡æ ‡
        
        Args:
            metrics: calculate_metrics è¿”å›çš„æŒ‡æ ‡å­—å…¸
            model_name: æ¨¡å‹åç§°
        """
        print("\n" + "=" * 70)
        print(f"ğŸ“Š {model_name} è¯„ä¼°ç»“æœ")
        print("=" * 70)
        
        print(f"\nã€æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‘")
        print(f"  âœ“ Accuracy (å‡†ç¡®ç‡):          {metrics['accuracy']*100:.2f}%")
        print(f"  âœ“ Precision (ç²¾ç¡®ç‡ - Macro):  {metrics['precision_macro']*100:.2f}%")
        print(f"  âœ“ Recall (å¬å›ç‡ - Macro):     {metrics['recall_macro']*100:.2f}%")
        print(f"  âœ“ F1-Score (Macro):            {metrics['f1_macro']*100:.2f}%")
        
        print(f"\nã€åŠ æƒå¹³å‡æŒ‡æ ‡ã€‘")
        print(f"  âœ“ Precision (Weighted):        {metrics['precision_weighted']*100:.2f}%")
        print(f"  âœ“ Recall (Weighted):           {metrics['recall_weighted']*100:.2f}%")
        print(f"  âœ“ F1-Score (Weighted):         {metrics['f1_weighted']*100:.2f}%")
        
        print(f"\nã€å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘")
        print(f"{'ç±»åˆ«':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
        print("-" * 50)
        for i, class_name in enumerate(self.class_names):
            precision = metrics['precision_per_class'][i] * 100
            recall = metrics['recall_per_class'][i] * 100
            f1 = metrics['f1_per_class'][i] * 100
            print(f"{class_name:<10} {precision:>6.2f}%      {recall:>6.2f}%      {f1:>6.2f}%")
        
        print("=" * 70 + "\n")
    
    def plot_confusion_matrix(self, confusion_matrix, save_path=None, model_name="Model"):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        
        Args:
            confusion_matrix: æ··æ·†çŸ©é˜µ
            save_path: ä¿å­˜è·¯å¾„
            model_name: æ¨¡å‹åç§°
        """
        plt.figure(figsize=(10, 8))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names,
                    cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
        
        plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ', fontsize=14, pad=20)
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def plot_metrics_comparison(self, metrics_dict, save_path=None):
        """
        ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„æŒ‡æ ‡å¯¹æ¯”å›¾
        
        Args:
            metrics_dict: {model_name: metrics} å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        model_names = list(metrics_dict.keys())
        num_models = len(model_names)
        
        # å‡†å¤‡æ•°æ®
        accuracies = [metrics_dict[m]['accuracy'] * 100 for m in model_names]
        precisions = [metrics_dict[m]['precision_macro'] * 100 for m in model_names]
        recalls = [metrics_dict[m]['recall_macro'] * 100 for m in model_names]
        f1_scores = [metrics_dict[m]['f1_macro'] * 100 for m in model_names]
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # é¢œè‰²
        colors = plt.cm.Set3(range(num_models))
        
        # Accuracy
        axes[0, 0].bar(model_names, accuracies, color=colors, alpha=0.8)
        axes[0, 0].set_title('Accuracy (å‡†ç¡®ç‡)', fontsize=12, fontweight='bold')
        axes[0, 0].set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=10)
        axes[0, 0].set_ylim([0, 100])
        axes[0, 0].grid(axis='y', alpha=0.3)
        for i, v in enumerate(accuracies):
            axes[0, 0].text(i, v + 2, f'{v:.2f}%', ha='center', fontsize=9)
        
        # Precision
        axes[0, 1].bar(model_names, precisions, color=colors, alpha=0.8)
        axes[0, 1].set_title('Precision (ç²¾ç¡®ç‡ - Macro)', fontsize=12, fontweight='bold')
        axes[0, 1].set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=10)
        axes[0, 1].set_ylim([0, 100])
        axes[0, 1].grid(axis='y', alpha=0.3)
        for i, v in enumerate(precisions):
            axes[0, 1].text(i, v + 2, f'{v:.2f}%', ha='center', fontsize=9)
        
        # Recall
        axes[1, 0].bar(model_names, recalls, color=colors, alpha=0.8)
        axes[1, 0].set_title('Recall (å¬å›ç‡ - Macro)', fontsize=12, fontweight='bold')
        axes[1, 0].set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=10)
        axes[1, 0].set_ylim([0, 100])
        axes[1, 0].grid(axis='y', alpha=0.3)
        for i, v in enumerate(recalls):
            axes[1, 0].text(i, v + 2, f'{v:.2f}%', ha='center', fontsize=9)
        
        # F1-Score
        axes[1, 1].bar(model_names, f1_scores, color=colors, alpha=0.8)
        axes[1, 1].set_title('F1-Score (Macro)', fontsize=12, fontweight='bold')
        axes[1, 1].set_ylabel('ç™¾åˆ†æ¯” (%)', fontsize=10)
        axes[1, 1].set_ylim([0, 100])
        axes[1, 1].grid(axis='y', alpha=0.3)
        for i, v in enumerate(f1_scores):
            axes[1, 1].text(i, v + 2, f'{v:.2f}%', ha='center', fontsize=9)
        
        plt.suptitle('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def save_metrics_to_file(self, metrics, model_name, save_path):
        """
        å°†è¯„ä¼°æŒ‡æ ‡ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
        
        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
            model_name: æ¨¡å‹åç§°
            save_path: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write(f"{model_name} è¯„ä¼°ç»“æœ\n")
            f.write("=" * 70 + "\n\n")
            
            f.write("ã€æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‘\n")
            f.write(f"  Accuracy (å‡†ç¡®ç‡):          {metrics['accuracy']*100:.2f}%\n")
            f.write(f"  Precision (ç²¾ç¡®ç‡ - Macro):  {metrics['precision_macro']*100:.2f}%\n")
            f.write(f"  Recall (å¬å›ç‡ - Macro):     {metrics['recall_macro']*100:.2f}%\n")
            f.write(f"  F1-Score (Macro):            {metrics['f1_macro']*100:.2f}%\n\n")
            
            f.write("ã€åŠ æƒå¹³å‡æŒ‡æ ‡ã€‘\n")
            f.write(f"  Precision (Weighted):        {metrics['precision_weighted']*100:.2f}%\n")
            f.write(f"  Recall (Weighted):           {metrics['recall_weighted']*100:.2f}%\n")
            f.write(f"  F1-Score (Weighted):         {metrics['f1_weighted']*100:.2f}%\n\n")
            
            f.write("ã€å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘\n")
            f.write(f"{'ç±»åˆ«':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\n")
            f.write("-" * 50 + "\n")
            for i, class_name in enumerate(self.class_names):
                precision = metrics['precision_per_class'][i] * 100
                recall = metrics['recall_per_class'][i] * 100
                f1 = metrics['f1_per_class'][i] * 100
                f.write(f"{class_name:<10} {precision:>6.2f}%      {recall:>6.2f}%      {f1:>6.2f}%\n")
            
            f.write("\nã€æ··æ·†çŸ©é˜µã€‘\n")
            f.write(str(metrics['confusion_matrix']) + "\n")
            f.write("=" * 70 + "\n")
        
        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")


def calculate_class_statistics(y_true, y_pred, num_classes=4):
    """
    è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°é‡
        
    Returns:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    if torch.is_tensor(y_true):
        y_true = y_true.cpu().numpy()
    if torch.is_tensor(y_pred):
        y_pred = y_pred.cpu().numpy()
    
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()
    
    stats = {}
    for c in range(num_classes):
        true_mask = (y_true == c)
        pred_mask = (y_pred == c)
        
        # True Positive, False Positive, False Negative, True Negative
        tp = np.sum(true_mask & pred_mask)
        fp = np.sum(~true_mask & pred_mask)
        fn = np.sum(true_mask & ~pred_mask)
        tn = np.sum(~true_mask & ~pred_mask)
        
        total = np.sum(true_mask)
        correct = np.sum(true_mask & pred_mask)
        
        stats[c] = {
            'total': int(total),
            'correct': int(correct),
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn),
            'tn': int(tn),
            'accuracy': correct / total if total > 0 else 0
        }
    
    return stats
