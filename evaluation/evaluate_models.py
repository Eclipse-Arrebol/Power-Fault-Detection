"""
è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—å„é¡¹æŒ‡æ ‡
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from sklearn.model_selection import train_test_split

from src.dataset import PowerGridDataset
from src.models import GCN, TGCN, NeuralGrangerCausality, CausalGCN_LSTM
from evaluation.metrics import ModelEvaluator, calculate_class_statistics


def evaluate_gcn(dataset, evaluator, device):
    """
    è¯„ä¼° GCN æ¨¡å‹
    """
    print("\n" + "="*70)
    print("ğŸ” è¯„ä¼° GCN æ¨¡å‹")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = "result/gcn/checkpoint.pth"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½æ•°æ®
    data_list = dataset.get_pyg_data_list()
    train_data, test_data = train_test_split(data_list, test_size=0.2, shuffle=False)
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    model = GCN(num_features=6, num_classes=4).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']})")
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    y_true_list = []
    y_pred_list = []
    
    with torch.no_grad():
        for data in test_data:
            data = data.to(device)
            out = model(data)
            pred = out.argmax(dim=1)
            
            # åªç»Ÿè®¡æœ‰æ•ˆèŠ‚ç‚¹
            mask = data.train_mask
            y_true_list.append(data.y[mask].cpu())
            y_pred_list.append(pred[mask].cpu())
    
    y_true = torch.cat(y_true_list).numpy()
    y_pred = torch.cat(y_pred_list).numpy()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_metrics(y_true, y_pred)
    evaluator.print_metrics(metrics, model_name="GCN")
    
    # ä¿å­˜ç»“æœ
    save_dir = "result/gcn"
    evaluator.save_metrics_to_file(metrics, "GCN", f"{save_dir}/evaluation_report.txt")
    evaluator.plot_confusion_matrix(metrics['confusion_matrix'], 
                                     save_path=f"{save_dir}/confusion_matrix.png",
                                     model_name="GCN")
    
    return metrics


def evaluate_tgcn(dataset, evaluator, device, seq_len=12, batch_size=64):
    """
    è¯„ä¼° TGCN æ¨¡å‹
    """
    print("\n" + "="*70)
    print("ğŸ” è¯„ä¼° TGCN æ¨¡å‹")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = "result/tgcn/checkpoint.pth"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½æ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=seq_len)
    num_samples = X.shape[0]
    num_features = X.shape[3]
    
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
    edge_index = edge_index.to(device)
    edge_weight = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    model = TGCN(num_features=num_features, num_classes=4, hidden_dim=64, num_layers=2).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']})")
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    y_true_list = []
    y_pred_list = []
    
    with torch.no_grad():
        for i in range(0, len(test_idx), batch_size):
            end_idx = min(i + batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            out = model(x_batch, edge_index, edge_weight)
            pred = out.argmax(dim=2)
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            y_true_list.append(y_flat[mask_expanded].cpu())
            y_pred_list.append(pred_flat[mask_expanded].cpu())
    
    y_true = torch.cat(y_true_list).numpy()
    y_pred = torch.cat(y_pred_list).numpy()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_metrics(y_true, y_pred)
    evaluator.print_metrics(metrics, model_name="TGCN")
    
    # ä¿å­˜ç»“æœ
    save_dir = "result/tgcn"
    evaluator.save_metrics_to_file(metrics, "TGCN", f"{save_dir}/evaluation_report.txt")
    evaluator.plot_confusion_matrix(metrics['confusion_matrix'], 
                                     save_path=f"{save_dir}/confusion_matrix.png",
                                     model_name="TGCN")
    
    return metrics


def evaluate_ngc(dataset, evaluator, device, seq_len=12, batch_size=64):
    """
    è¯„ä¼° NGC æ¨¡å‹
    """
    print("\n" + "="*70)
    print("ğŸ” è¯„ä¼° NGC æ¨¡å‹")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = "result/ngc/checkpoint.pth"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½æ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=seq_len)
    num_samples = X.shape[0]
    num_nodes = X.shape[2]
    num_features = X.shape[3]
    
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
    edge_index = edge_index.to(device)
    edge_weight = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    model = NeuralGrangerCausality(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=4,
        hidden_dim=64,
        num_layers=2,
        sparsity_lambda=checkpoint.get('sparsity_lambda', 0.01)
    ).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']})")
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    y_true_list = []
    y_pred_list = []
    
    with torch.no_grad():
        for i in range(0, len(test_idx), batch_size):
            end_idx = min(i + batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            out = model(x_batch, edge_index, edge_weight)
            pred = out.argmax(dim=2)
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            y_true_list.append(y_flat[mask_expanded].cpu())
            y_pred_list.append(pred_flat[mask_expanded].cpu())
    
    y_true = torch.cat(y_true_list).numpy()
    y_pred = torch.cat(y_pred_list).numpy()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_metrics(y_true, y_pred)
    evaluator.print_metrics(metrics, model_name="NGC")
    
    # ä¿å­˜ç»“æœ
    save_dir = "result/ngc"
    evaluator.save_metrics_to_file(metrics, "NGC", f"{save_dir}/evaluation_report.txt")
    evaluator.plot_confusion_matrix(metrics['confusion_matrix'], 
                                     save_path=f"{save_dir}/confusion_matrix.png",
                                     model_name="NGC")
    
    return metrics


def evaluate_causal_gcn_lstm(dataset, evaluator, device, seq_len=12, batch_size=64):
    """
    è¯„ä¼° CausalGCN_LSTM æ¨¡å‹
    """
    print("\n" + "="*70)
    print("ğŸ” è¯„ä¼° CausalGCN_LSTM æ¨¡å‹")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = "result/causal_gcn_lstm/checkpoint.pth"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½æ•°æ®
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=seq_len)
    num_samples = X.shape[0]
    num_nodes = X.shape[2]
    num_features = X.shape[3]
    
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
    edge_index_tensor = edge_index.to(device)
    edge_weight_tensor = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    edge_index_np = edge_index.numpy()
    
    model = CausalGCN_LSTM(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=4,
        edge_index=edge_index_np,
        admittance_matrix=None,
        source_node=0,
        gcn_hidden=checkpoint['config']['gcn_hidden'],
        lstm_hidden=checkpoint['config']['lstm_hidden'],
        num_gcn_layers=checkpoint['config']['num_gcn_layers'],
        num_lstm_layers=checkpoint['config']['num_lstm_layers'],
        dropout=0.3
    ).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']})")
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    y_true_list = []
    y_pred_list = []
    
    with torch.no_grad():
        for i in range(0, len(test_idx), batch_size):
            end_idx = min(i + batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            outputs = model(x_batch)
            anomaly_logits = outputs['anomaly_logits']
            pred = anomaly_logits.argmax(dim=2)
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            y_true_list.append(y_flat[mask_expanded].cpu())
            y_pred_list.append(pred_flat[mask_expanded].cpu())
    
    y_true = torch.cat(y_true_list).numpy()
    y_pred = torch.cat(y_pred_list).numpy()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = evaluator.calculate_metrics(y_true, y_pred)
    evaluator.print_metrics(metrics, model_name="CausalGCN_LSTM")
    
    # ä¿å­˜ç»“æœ
    save_dir = "result/causal_gcn_lstm"
    evaluator.save_metrics_to_file(metrics, "CausalGCN_LSTM", f"{save_dir}/evaluation_report.txt")
    evaluator.plot_confusion_matrix(metrics['confusion_matrix'], 
                                     save_path=f"{save_dir}/confusion_matrix.png",
                                     model_name="CausalGCN_LSTM")
    
    return metrics


def main():
    """
    ä¸»å‡½æ•°ï¼šè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    """
    print("\n" + "ğŸ¯" * 30)
    print("å¼€å§‹è¯„ä¼°æ‰€æœ‰æ¨¡å‹")
    print("ğŸ¯" * 30 + "\n")
    
    # åˆå§‹åŒ–è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("\nåŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(class_names=['æ­£å¸¸', 'çªå¢', 'ä¸¢å¤±', 'æ— åŠŸ'])
    
    # è¯„ä¼°å„ä¸ªæ¨¡å‹
    all_metrics = {}
    
    # 1. è¯„ä¼° GCN
    metrics = evaluate_gcn(dataset, evaluator, device)
    if metrics:
        all_metrics['GCN'] = metrics
    
    # 2. è¯„ä¼° TGCN
    metrics = evaluate_tgcn(dataset, evaluator, device)
    if metrics:
        all_metrics['TGCN'] = metrics
    
    # 3. è¯„ä¼° NGC
    metrics = evaluate_ngc(dataset, evaluator, device)
    if metrics:
        all_metrics['NGC'] = metrics
    
    # 4. è¯„ä¼° CausalGCN_LSTM
    metrics = evaluate_causal_gcn_lstm(dataset, evaluator, device)
    if metrics:
        all_metrics['CausalGCN_LSTM'] = metrics
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    if len(all_metrics) > 0:
        print("\n" + "="*70)
        print("ğŸ“Š ç”Ÿæˆæ¨¡å‹å¯¹æ¯”å›¾...")
        print("="*70)
        evaluator.plot_metrics_comparison(all_metrics, save_path="result/model_comparison.png")
        
        # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        save_comparison_report(all_metrics, "result/comparison_report.txt")
    
    print("\n" + "âœ…" * 30)
    print("æ‰€æœ‰æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
    print("âœ…" * 30 + "\n")


def save_comparison_report(metrics_dict, save_path):
    """
    ä¿å­˜æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # è¡¨å¤´
        f.write(f"{'æ¨¡å‹':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\n")
        f.write("-" * 80 + "\n")
        
        # æ¯ä¸ªæ¨¡å‹çš„æ•°æ®
        for model_name, metrics in metrics_dict.items():
            acc = metrics['accuracy'] * 100
            prec = metrics['precision_macro'] * 100
            rec = metrics['recall_macro'] * 100
            f1 = metrics['f1_macro'] * 100
            
            f.write(f"{model_name:<20} {acc:>6.2f}%      {prec:>6.2f}%      {rec:>6.2f}%      {f1:>6.2f}%\n")
        
        f.write("\n" + "=" * 80 + "\n")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_acc_model = max(metrics_dict.items(), key=lambda x: x[1]['accuracy'])
        best_f1_model = max(metrics_dict.items(), key=lambda x: x[1]['f1_macro'])
        
        f.write("\nã€æœ€ä½³æ¨¡å‹ã€‘\n")
        f.write(f"  æœ€é«˜å‡†ç¡®ç‡: {best_acc_model[0]} ({best_acc_model[1]['accuracy']*100:.2f}%)\n")
        f.write(f"  æœ€é«˜F1åˆ†æ•°: {best_f1_model[0]} ({best_f1_model[1]['f1_macro']*100:.2f}%)\n")
        
        f.write("\n" + "=" * 80 + "\n")
    
    print(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")


if __name__ == "__main__":
    main()
