"""
æ”¹è¿›ç‰ˆè®­ç»ƒè„šæœ¬ - é’ˆå¯¹"ä¸¢å¤±"ç±»å‹åˆ†ç±»ä¼˜åŒ–

ä¸»è¦æ”¹è¿›ï¼š
1. å¯è°ƒæ•´çš„ç±»åˆ«æƒé‡é…ç½®
2. Focal Loss å‚æ•°å®éªŒ
3. åå¤„ç†é˜ˆå€¼è°ƒæ•´
"""
import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import os

from src.dataset import PowerGridDataset
from src.models import CausalGCN_LSTM
from src.loss.causal_loss import create_causal_loss
from plot.causal_plot import plot_causal_graph, analyze_causal_structure

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# ============================================================
# ğŸ”§ å®éªŒé…ç½®ï¼šæ”¹è¿›"ä¸¢å¤±"ç±»å‹åˆ†ç±»
# ============================================================
WEIGHT_CONFIGS = {
    'original': [2.0, 50.0, 50.0, 20.0],        # åŸå§‹æƒé‡
    'reduce_missing': [2.0, 50.0, 25.0, 20.0],  # é™ä½ä¸¢å¤±æƒé‡ï¼ˆæ¨èï¼‰
    'balanced': [2.0, 30.0, 30.0, 20.0],        # å¹³è¡¡æƒé‡
    'aggressive': [2.0, 50.0, 15.0, 20.0]       # æ¿€è¿›é™ä½ä¸¢å¤±æƒé‡
}

# é€‰æ‹©æƒé‡é…ç½®
WEIGHT_CONFIG = 'reduce_missing'  # æ”¹è¿™é‡Œé€‰æ‹©ä¸åŒé…ç½®

# Focal Loss å‚æ•°
FOCAL_GAMMA = 2.5  # é»˜è®¤2.0ï¼Œå¢åŠ åˆ°2.5æˆ–3.0æ›´å…³æ³¨éš¾æ ·æœ¬

# å…¶ä»–é…ç½®
SEQ_LEN = 12
BATCH_SIZE = 32
NUM_EPOCHS = 100


def predict_with_threshold(logits, missing_threshold=0.7):
    """
    ä½¿ç”¨é˜ˆå€¼è°ƒæ•´é¢„æµ‹ï¼Œå‡å°‘"ä¸¢å¤±"ç±»çš„è¯¯åˆ¤
    
    Args:
        logits: [B, N, C] æ¨¡å‹è¾“å‡º
        missing_threshold: ä¸¢å¤±ç±»çš„æ¦‚ç‡é˜ˆå€¼ï¼Œé»˜è®¤0.7
        
    Returns:
        pred: è°ƒæ•´åçš„é¢„æµ‹ç»“æœ
    """
    probs = torch.softmax(logits, dim=-1)  # [B, N, C]
    pred = logits.argmax(dim=-1)  # åŸå§‹é¢„æµ‹
    
    # å¯¹é¢„æµ‹ä¸º"ä¸¢å¤±"(class 2)ä½†æ¦‚ç‡ä¸å¤Ÿé«˜çš„ï¼Œé‡æ–°åˆ†ç±»
    class_2_mask = (pred == 2)
    class_2_prob = probs[:, :, 2]
    low_confidence = class_2_mask & (class_2_prob < missing_threshold)
    
    if low_confidence.any():
        # å±è”½ä¸¢å¤±ç±»ï¼Œé€‰æ‹©æ¬¡ä¼˜ç±»åˆ«
        probs_copy = probs.clone()
        probs_copy[:, :, 2] = -float('inf')
        alternative_pred = probs_copy.argmax(dim=-1)
        pred[low_confidence] = alternative_pred[low_confidence]
    
    return pred


def train_improved_causal_gcn_lstm():
    """
    æ”¹è¿›ç‰ˆ CausalGCN_LSTM è®­ç»ƒ
    """
    print("=" * 60)
    print("ğŸ§  æ”¹è¿›ç‰ˆ CausalGCN_LSTM è®­ç»ƒ")
    print(f"   é…ç½®: {WEIGHT_CONFIG}")
    print(f"   æƒé‡: {WEIGHT_CONFIGS[WEIGHT_CONFIG]}")
    print(f"   Focal Gamma: {FOCAL_GAMMA}")
    print("=" * 60)
    
    # 1. å‡†å¤‡æ•°æ®
    print("\n>>> [1/6] åŠ è½½æ•°æ®é›†...")
    dataset = PowerGridDataset(dataset_path="dataset")
    
    X, Y, edge_index, edge_weight, node_mask = dataset.get_temporal_tensors(seq_len=SEQ_LEN)
    num_samples = X.shape[0]
    num_nodes = X.shape[2]
    num_features = X.shape[3]
    
    print(f">>> æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f">>> èŠ‚ç‚¹æ•°: {num_nodes}, ç‰¹å¾æ•°: {num_features}")
    
    # ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°
    print("\n>>> æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ:")
    for c in range(4):
        count = (Y == c).sum().item()
        print(f"    Class {c}: {count} æ ·æœ¬")
    
    # 2. åˆå§‹åŒ–è®¾å¤‡
    use_cuda = torch.cuda.is_available()
    device = torch.device('cuda' if use_cuda else 'cpu')
    if use_cuda:
        print(f"\n>>> æ£€æµ‹åˆ° CUDA: {torch.version.cuda}")
    else:
        print("\n>>> ä½¿ç”¨ CPU è®­ç»ƒ")
    
    edge_index_tensor = edge_index.to(device)
    edge_weight_tensor = edge_weight.to(device)
    node_mask = node_mask.to(device)
    
    # 3. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    indices = list(range(num_samples))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, shuffle=False)
    
    X_train, Y_train = X[train_idx], Y[train_idx]
    X_test, Y_test = X[test_idx], Y[test_idx]
    
    print(f"\n>>> è®­ç»ƒé›†: {len(train_idx)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_idx)} æ ·æœ¬")
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    print(f"\n>>> [2/6] åˆå§‹åŒ–æ¨¡å‹...")
    edge_index_np = edge_index.numpy()
    
    model = CausalGCN_LSTM(
        num_nodes=num_nodes,
        num_features=num_features,
        num_classes=4,
        edge_index=edge_index_np,
        admittance_matrix=None,
        source_node=0,
        gcn_hidden=64,
        lstm_hidden=128,
        num_gcn_layers=2,
        num_lstm_layers=2,
        dropout=0.3
    ).to(device)
    
    # 5. åˆå§‹åŒ–æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨è°ƒæ•´åçš„æƒé‡ï¼‰
    print(f"\n>>> [3/6] åˆå§‹åŒ–æŸå¤±å‡½æ•°...")
    class_weights = torch.tensor(WEIGHT_CONFIGS[WEIGHT_CONFIG]).to(device)
    print(f">>> ç±»åˆ«æƒé‡: {class_weights.cpu().tolist()}")
    
    node_depths = model.node_depths.clone()
    
    criterion = create_causal_loss(
        class_weights=class_weights,
        node_depths=node_depths,
        use_focal_loss=True,
        focal_gamma=FOCAL_GAMMA,
        lambda_root=0.5,
        lambda_sparse=0.01,
        lambda_physics=0.1
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
    
    # 6. è®­ç»ƒå¾ªç¯
    print(f"\n>>> [4/6] å¼€å§‹è®­ç»ƒ...")
    loss_history = {'total': [], 'anomaly': [], 'sparse': []}
    class_metrics = {'precision': [], 'recall': [], 'f1': []}
    
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        total_anomaly_loss = 0
        total_sparse_loss = 0
        
        perm = torch.randperm(len(train_idx))
        for i in range(0, len(train_idx), BATCH_SIZE):
            batch_indices = perm[i:i + BATCH_SIZE]
            current_batch_size = len(batch_indices)
            
            x_batch = X_train[batch_indices].to(device, non_blocking=True)
            y_batch = Y_train[batch_indices].to(device, non_blocking=True)
            
            optimizer.zero_grad()
            outputs = model(x_batch, return_causal=True)
            
            loss, loss_dict = criterion(
                model_outputs=outputs,
                anomaly_labels=y_batch,
                root_cause_labels=None,
                x=x_batch,
                model=model
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss_dict.get('total', loss.item())
            total_anomaly_loss += loss_dict.get('anomaly', 0)
            total_sparse_loss += loss_dict.get('sparse', 0)
        
        scheduler.step()
        
        num_batches = len(train_idx) // BATCH_SIZE + 1
        avg_loss = total_loss / num_batches
        avg_anomaly = total_anomaly_loss / num_batches
        avg_sparse = total_sparse_loss / num_batches
        
        loss_history['total'].append(avg_loss)
        loss_history['anomaly'].append(avg_anomaly)
        loss_history['sparse'].append(avg_sparse)
        
        # æ¯10ä¸ªepochè¯„ä¼°ä¸€æ¬¡"ä¸¢å¤±"ç±»çš„æ€§èƒ½
        if epoch % 10 == 0:
            model.eval()
            with torch.no_grad():
                # åœ¨éªŒè¯é›†ä¸Šå¿«é€Ÿè¯„ä¼°
                val_size = min(len(test_idx), 500)
                x_val = X_test[:val_size].to(device)
                y_val = Y_test[:val_size].to(device)
                
                outputs = model(x_val)
                anomaly_logits = outputs['anomaly_logits']
                
                # ä½¿ç”¨é˜ˆå€¼è°ƒæ•´çš„é¢„æµ‹
                pred = predict_with_threshold(anomaly_logits, missing_threshold=0.7)
                
                # è®¡ç®—"ä¸¢å¤±"ç±»çš„æŒ‡æ ‡
                mask = node_mask.repeat(val_size).cpu()
                pred_flat = pred.view(-1).cpu()
                y_flat = y_val.view(-1).cpu()
                
                # åªç»Ÿè®¡"ä¸¢å¤±"ç±» (class 2)
                class_2_mask = (y_flat == 2) & mask
                if class_2_mask.sum() > 0:
                    tp = ((pred_flat == 2) & (y_flat == 2) & mask).sum().item()
                    fp = ((pred_flat == 2) & (y_flat != 2) & mask).sum().item()
                    fn = ((pred_flat != 2) & (y_flat == 2) & mask).sum().item()
                    
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                    
                    print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f} | "
                          f"ä¸¢å¤±ç±» - P: {precision*100:.1f}% R: {recall*100:.1f}% F1: {f1*100:.1f}%")
                else:
                    print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f}")
            
            model.train()
    
    # 7. æœ€ç»ˆè¯„ä¼°
    print("\n>>> [5/6] è¯„ä¼°æ¨¡å‹...")
    model.eval()
    correct = 0
    total = 0
    class_correct = [0, 0, 0, 0]
    class_total = [0, 0, 0, 0]
    class_tp = [0, 0, 0, 0]
    class_fp = [0, 0, 0, 0]
    class_fn = [0, 0, 0, 0]
    
    test_batch_size = BATCH_SIZE * 2
    with torch.no_grad():
        for i in range(0, len(test_idx), test_batch_size):
            end_idx = min(i + test_batch_size, len(test_idx))
            batch_indices = range(i, end_idx)
            current_batch_size = len(batch_indices)
            
            x_batch = X_test[batch_indices].to(device)
            y_batch = Y_test[batch_indices].to(device)
            
            outputs = model(x_batch)
            anomaly_logits = outputs['anomaly_logits']
            
            # ğŸ”¥ ä½¿ç”¨é˜ˆå€¼è°ƒæ•´çš„é¢„æµ‹
            pred = predict_with_threshold(anomaly_logits, missing_threshold=0.7)
            
            mask_expanded = node_mask.repeat(current_batch_size)
            pred_flat = pred.view(-1)
            y_flat = y_batch.view(-1)
            
            valid_mask = mask_expanded
            correct += (pred_flat[valid_mask] == y_flat[valid_mask]).sum().item()
            total += valid_mask.sum().item()
            
            for c in range(4):
                c_mask = valid_mask & (y_flat == c)
                class_total[c] += c_mask.sum().item()
                class_correct[c] += (pred_flat[c_mask] == y_flat[c_mask]).sum().item()
                
                # è®¡ç®— TP, FP, FN
                class_tp[c] += ((pred_flat == c) & (y_flat == c) & valid_mask).sum().item()
                class_fp[c] += ((pred_flat == c) & (y_flat != c) & valid_mask).sum().item()
                class_fn[c] += ((pred_flat != c) & (y_flat == c) & valid_mask).sum().item()
    
    acc = correct / total if total > 0 else 0
    
    print("=" * 60)
    print(f"âœ… æµ‹è¯•é›†æ€»å‡†ç¡®ç‡: {acc * 100:.2f}%")
    print("\nã€å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘")
    class_names = ['æ­£å¸¸', 'çªå¢', 'ä¸¢å¤±', 'æ— åŠŸ']
    
    for c in range(4):
        accuracy = 100 * class_correct[c] / max(class_total[c], 1)
        precision = 100 * class_tp[c] / max(class_tp[c] + class_fp[c], 1)
        recall = 100 * class_tp[c] / max(class_tp[c] + class_fn[c], 1)
        f1 = 2 * precision * recall / max(precision + recall, 1)
        
        marker = "ğŸ¯" if c == 2 else "  "
        print(f"{marker} {class_names[c]}: Acc={accuracy:.1f}% | P={precision:.1f}% | R={recall:.1f}% | F1={f1:.1f}%")
    
    print("=" * 60)
    
    # ä¿å­˜æ¨¡å‹
    save_dir = f"result/improved_{WEIGHT_CONFIG}"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    torch.save(model.state_dict(), f"{save_dir}/model.pth")
    checkpoint = {
        'epoch': NUM_EPOCHS,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
        'accuracy': acc,
        'config': {
            'weight_config': WEIGHT_CONFIG,
            'class_weights': WEIGHT_CONFIGS[WEIGHT_CONFIG],
            'focal_gamma': FOCAL_GAMMA,
            'missing_threshold': 0.7
        }
    }
    torch.save(checkpoint, f"{save_dir}/checkpoint.pth")
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}/")
    
    # ç»˜åˆ¶å› æœå›¾
    print("\n>>> [6/6] ç»˜åˆ¶å› æœå›¾...")
    causal_matrix = model.get_causal_matrix().detach().cpu().numpy()
    np.save(f"{save_dir}/causal_matrix.npy", causal_matrix)
    
    analyze_causal_structure(causal_matrix, top_k=10)
    plot_causal_graph(causal_matrix, threshold=0.1, save_path=f"{save_dir}/causal_graph.png")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    axes[0].plot(loss_history['total'], color='blue')
    axes[0].set_title(f"Total Loss ({WEIGHT_CONFIG})")
    axes[0].set_xlabel("Epoch")
    axes[0].grid(True)
    
    axes[1].plot(loss_history['anomaly'], color='green')
    axes[1].set_title("Anomaly Loss")
    axes[1].set_xlabel("Epoch")
    axes[1].grid(True)
    
    axes[2].plot(loss_history['sparse'], color='orange')
    axes[2].set_title("Sparsity Loss")
    axes[2].set_xlabel("Epoch")
    axes[2].grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/training_loss.png", dpi=300)
    plt.show()
    
    return model, acc


if __name__ == "__main__":
    print("\n" + "ğŸ¯" * 30)
    print("æ”¹è¿›ç‰ˆè®­ç»ƒï¼šé’ˆå¯¹'ä¸¢å¤±'ç±»å‹ä¼˜åŒ–")
    print("ğŸ¯" * 30)
    
    train_improved_causal_gcn_lstm()
