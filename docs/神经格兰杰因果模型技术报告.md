# 神经格兰杰因果模型 (Neural Granger Causality) 技术报告

> **项目**: 电网负荷异常检测  
> **日期**: 2026年1月13日  
> **版本**: v1.0

---

## 一、背景与动机

在电力系统中，各节点（母线）之间存在复杂的电气耦合关系。当某个节点发生异常时，这种异常可能通过网络拓扑传播到其他节点。为了更好地理解这种**因果传播机制**，我们引入了**神经格兰杰因果分析**方法。

### 1.1 什么是格兰杰因果？

格兰杰因果 (Granger Causality) 是一种基于时间序列的因果推断方法：

> 如果变量 X 的历史信息能够帮助预测变量 Y 的未来值，则称 X 对 Y 存在格兰杰因果关系。

**数学定义**：
$$
Y_t = f(Y_{t-1}, ..., Y_{t-p}) + \epsilon_1 \\
Y_t = g(Y_{t-1}, ..., Y_{t-p}, X_{t-1}, ..., X_{t-p}) + \epsilon_2
$$

若 $\text{Var}(\epsilon_2) < \text{Var}(\epsilon_1)$，则 X → Y 存在格兰杰因果。

### 1.2 为什么需要神经格兰杰因果？

传统格兰杰因果基于线性回归，无法捕捉：
- **非线性因果关系**
- **高维多变量交互**
- **复杂时空耦合**

神经格兰杰因果 (NGC) 使用神经网络学习非线性映射，并通过**稀疏性正则化**自动发现因果结构。

---

## 二、模型架构

### 2.1 整体架构

```
输入序列 [B, T, N, F]
        ↓
    输入编码层 (Linear)
        ↓x
  ┌─────────────────┐
  │  因果注意力层     │  ← 可学习因果矩阵 W
  └─────────────────┘
        ↓
    GCN 空间特征提取 (多层)
        ↓
    GRU 时间序列建模
        ↓
    输出分类层
        ↓
   类别预测 [B, N, C]
```

### 2.2 核心组件

#### (1) 可学习因果权重矩阵

```python
self.causal_weight = nn.Parameter(torch.randn(num_nodes, num_nodes) * 0.01)
```

- **矩阵维度**: `[N, N]`，N 为节点数
- **物理含义**: `W[i, j]` 表示节点 j 对节点 i 的因果影响强度
- **初始化**: 小随机值，让网络自己学习稀疏结构

#### (2) 因果注意力机制

```python
def apply_causal_attention(self, x):
    W = torch.softmax(self.causal_weight, dim=1)  # 归一化
    x_agg = torch.matmul(W, x)  # 因果聚合
    x_concat = torch.cat([x, x_agg], dim=-1)
    return self.causal_fusion(x_concat)
```

通过因果权重对邻居特征进行加权聚合，融合原始特征和因果特征。

#### (3) 稀疏性正则化

```python
def get_sparsity_loss(self):
    return self.sparsity_lambda * torch.sum(torch.abs(self.causal_weight))
```

使用 **L1 范数** 促进因果矩阵的稀疏性，避免学到"全连接"的平凡解。

---

## 三、损失函数设计

### 3.1 总损失函数

$$
\mathcal{L}_{total} = \mathcal{L}_{cls} + \lambda \cdot \|W\|_1
$$

| 符号 | 含义 |
|------|------|
| $\mathcal{L}_{cls}$ | 分类交叉熵损失 (带类别权重) |
| $\lambda$ | 稀疏性正则化系数 |
| $\|W\|_1$ | 因果矩阵的 L1 范数 |

### 3.2 参数设置

```python
SPARSITY_LAMBDA = 0.01  # 稀疏性系数
class_weights = [2.0, 50.0, 50.0, 20.0]  # 类别权重
```

- **λ 过大**: 因果矩阵过于稀疏，可能丢失重要因果关系
- **λ 过小**: 因果矩阵过于稠密，无法区分重要因果边

---

## 四、实验结果可视化

### 4.1 因果关系网络图

训练完成后，系统自动生成因果关系图：

| 可视化内容 | 说明 |
|------------|------|
| **节点大小** | 入度（被影响程度） |
| **节点颜色** | 出度（影响力强度） |
| **边的粗细** | 因果强度 |
| **边的方向** | 因果方向（箭头指向被影响节点） |

### 4.2 因果矩阵热力图

- **横轴**: 原因节点 (Cause)
- **纵轴**: 结果节点 (Effect)
- **颜色深度**: 因果影响强度

### 4.3 因果结构分析

系统自动输出：
- **Top-K 最强因果关系**
- **最具影响力的节点**（因果源）
- **最易受影响的节点**（因果汇）
- **因果矩阵稀疏度统计**

---

## 五、代码修改清单

### 5.1 新增文件/函数

| 文件 | 新增内容 | 说明 |
|------|----------|------|
| `src/models.py` | `NeuralGrangerCausality` 类 | NGC 模型实现 |
| `train.py` | `train_ngc()` 函数 | NGC 训练流程 |
| `train.py` | `plot_causal_graph()` 函数 | 因果图绘制 |
| `train.py` | `analyze_causal_structure()` 函数 | 因果结构分析 |

### 5.2 配置参数

```python
# train.py 顶部配置
USE_NGC = True            # 启用神经格兰杰因果模型
USE_TEMPORAL = True       # 备选：时序GCN
SPARSITY_LAMBDA = 0.01    # 稀疏性正则化系数
SEQ_LEN = 12              # 时间窗口长度
BATCH_SIZE = 32           # 批大小
```

### 5.3 输出文件

| 输出路径 | 说明 |
|----------|------|
| `ngc_model.pth` | 模型权重 |
| `checkpoints/ngc_checkpoint.pth` | 完整检查点 |
| `img/causal_matrix.npy` | 因果矩阵（NumPy格式） |
| `img/causal_graph.png` | 因果关系可视化图 |
| `img/ngc_training_loss.png` | 训练损失曲线 |

---

## 六、使用方法

### 6.1 训练模型

```bash
# 确保配置 USE_NGC = True
python train.py
```

### 6.2 加载因果矩阵

```python
import numpy as np

# 加载因果矩阵
causal_matrix = np.load("img/causal_matrix.npy")

# 查看节点 i 受哪些节点影响
influences_on_i = causal_matrix[i, :]

# 查看节点 j 影响哪些节点
influenced_by_j = causal_matrix[:, j]
```

### 6.3 自定义阈值绘图

```python
from train import plot_causal_graph

# 使用不同阈值绘制因果图
plot_causal_graph(causal_matrix, threshold=0.2, save_path="img/causal_strict.png")
```

---

## 七、理论参考

1. **Tank, A., et al.** (2021). Neural Granger Causality. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.

2. **Marcinkevičs, R., & Vogt, J. E.** (2021). Interpretable Models for Granger Causality Using Self-Explaining Neural Networks. *ICLR*.

3. **Zhao, L., et al.** (2020). T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction. *IEEE Transactions on Intelligent Transportation Systems*.

---

## 八、后续优化方向

1. **多尺度因果分析**: 引入不同时间粒度的因果矩阵
2. **动态因果发现**: 让因果矩阵随时间变化
3. **因果干预实验**: 通过 do-calculus 进行反事实推理
4. **可解释性增强**: 结合注意力可视化解释因果关系

---

## 附录：模型参数统计

| 参数 | 值 |
|------|-----|
| 隐藏层维度 | 64 |
| GCN 层数 | 2 |
| GRU 层数 | 2 |
| Dropout | 0.3 |
| 学习率 | 0.003 |
| 权重衰减 | 5e-4 |
| 训练轮数 | 100 |

---

*报告生成时间: 2026-01-13*
